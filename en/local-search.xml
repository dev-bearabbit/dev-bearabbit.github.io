<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>My Experience Translating 『Julia for Data Analytics』</title>
    <link href="/en/en/Julia/julia-0/"/>
    <url>/en/en/Julia/julia-0/</url>
    
    <content type="html"><![CDATA[<p>In this post, I share my experience of translating an IT book for the first time — how I got started, the challenges I faced, and the lessons I learned along the way.</p><span id="more"></span><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>The book I translated was published in March 2024.<br>Although this reflection comes quite late, I only recently found the time to look back on the experience as I settled down from my hectic routine.<br>I’m not sure if I’ll have another chance to translate a book in the future, but just in case, I wanted to document both the difficulties and rewarding moments of the process.</p><p>Surprisingly, I was offered the opportunity to translate this book when the publisher reached out to me.<br>In 2020, I was studied in learning Julia and had written various blog posts about it.<br>It seems the publisher discovered my posts and decided to contact me.<br>Since it was a publisher whose IT books I had often purchased and read, I found the whole situation quite surreal.<br>There was no reason for me to turn down such a great opportunity.<br>I had a vague idea that it wouldn’t be hard to translate, because I’ve read so many tech articles and books in English.<br>Without much hesitation, I immediately replied to the publisher’s team leader, expressing my interest in the project.</p><p><img src="/en/images/6.jpeg" alt="Plz Think more"></p><p>I should have thought about it more carefully.</p><h2 id="Challenges-During-Translation"><a href="#Challenges-During-Translation" class="headerlink" title="Challenges During Translation"></a>Challenges During Translation</h2><p>At first, I assumed I could approach the translation the same way I studied technical materials.<br>However, this was the naive confidence of someone who knew nothing about translation.</p><p>The first challenge was converting English technical terms into appropriate Korean equivalents.<br>Many terms are commonly used in English within the industry, even in Korean-speaking contexts.<br>For example, should ecosystem and named tuple be translated, or should they remain in English?<br>How should I translate between parameter and argument in Korean?<br>When should I include both English and Korean terms together (i.e., provide a transliteration or explanation)?<br>Even choosing basic terms was more difficult than I thought.</p><p>English text is often structured with long, complex sentences, frequently using <code>which</code> or <code>that</code> to connect ideas.<br>In contrast, Korean generally flows better when broken into two or three separate sentences.<br>Additionally, English adjectives often follow nouns, whereas in Korean, they usually precede them.<br>If I translated English text word-for-word, I ended up with what I call <code>heavy-headed sentences</code> that were difficult to read.<br>However, if I broke up the sentences too much, I feared that I might alter the author’s intended meaning.<br>Early on, I sometimes spent 30 minutes agonizing over a single sentence.<br>But as I progressed, I naturally developed my own strategies and guidelines for handling these issues.</p><p>The third is something every office worker faces: the battle for time. Since my company is based at home, I thought I had a lot of time on my hands becuase i didn’t have to commute to the office. However, as an office worker, you have a busy day just managing your work and home. When work is busy, home is relaxed, and when work is relaxed, there are often incidents at home. I was no different. When I was busy at work, I was rushing to get things done, and when I was done, there were home issues that needed to be addressed, such as a sick family member who needed to be cared for.<br>Translation requires long, uninterrupted sessions to ensure a smooth and consistent tone across chapters.<br>With a looming deadline, I had to push through and complete the translation despite all these challenges.</p><h2 id="What-I-learned-from-translating"><a href="#What-I-learned-from-translating" class="headerlink" title="What I learned from translating"></a>What I learned from translating</h2><p>But if you ask me, “Do you regret working on the translation?”, of course not.<br>On the contrary, I feel honored to have been given this opportunity.</p><p>Translating forced me to revisit <code>basic concepts</code> that I had previously glossed over using industry jargon.<br>By putting these terms into my own words, I was able to internalize them more effectively and understand them much more deeply.<br>I think the saying “if you really want to understand something, explain it in your own words” is so true.</p><p>In the process, my knowledge of <code>Julia</code> has greatly improved.<br>Before this book, I learned Julia mainly by reading documentation and experimenting on my own.<br>But this book systematically explains Julia’s ecosystem and practical usage, introducing me to many concepts I hadn’t encountered before.<br>It also made me realize how much the language has evolved since I first started using it in 2020.</p><p>This experience gave me a glimpse into the world of <code>translation</code>, which I believe is a very rare and valuable opportunity.<br>I gained first-hand knowledge of how book translation projects are initiated, negotiated, and executed, and gained insights that I would not have gotten otherwise.</p><h2 id="Final-Thoughts"><a href="#Final-Thoughts" class="headerlink" title="Final Thoughts"></a>Final Thoughts</h2><p>I am so grateful that I was able to have this experience, even though I have no experience in translation and am in a junior career in some ways.<br>And also I’m grateful to the publisher and other people who worked with me on this project, even though I was a beginner in every aspect.<br>When the next translation opportunity comes, I will study a lot so that I can do a better translation.</p><p>For those of you who are curious about what the translated book is like, here is the <a href="https://blog.naver.com/jeipubmarketer/223381130430">book introduction page</a>.</p>]]></content>
    
    
    <categories>
      
      <category>Language</category>
      
      <category>julia</category>
      
    </categories>
    
    
    <tags>
      
      <tag>julia</tag>
      
      <tag>translate</tag>
      
      <tag>translation</tag>
      
      <tag>dataanalysis</tag>
      
      <tag>JuliaforDataAnalysis</tag>
      
      <tag>book</tag>
      
      <tag>korean</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Please Build a Data Warehouse</title>
    <link href="/en/en/Engineering/engineering-1/"/>
    <url>/en/en/Engineering/engineering-1/</url>
    
    <content type="html"><![CDATA[<p>As a data engineer, I’d like to share the importance of a proper data warehouse and how to construct one effectively, based on my experience.</p><span id="more"></span><h2 id="Introduction-Pouring-Water-into-a-Bottomless-Pit"><a href="#Introduction-Pouring-Water-into-a-Bottomless-Pit" class="headerlink" title="Introduction: Pouring Water into a Bottomless Pit"></a>Introduction: Pouring Water into a Bottomless Pit</h2><p>It’s already been four years since I started my career as a data engineer, and I’ve been working at my current company for almost three years now. When I first joined, the system relied heavily on duplicating RDS for creating data products. But over time, we’ve stabilized our data lake using the Medallion architecture, allowing us to focus on improving the quality of data products.</p><p>In this post, I’ll summarize the efforts made throughout 2024 to build a data warehouse and improve data quality.</p><p>From conversations with others, I’ve often heard the following reasons why data warehouses are perceived as a double-edged sword:</p><ul><li>Most data teams are small and too busy to handle such projects.</li><li>Setting up and properly using a data warehouse requires significant resources.</li><li>Adding another layer before the product feels unnecessarily cumbersome.</li><li>Some people still don’t fully understand why it’s needed given the resources required.</li></ul><p>So some companies didn’t have one at all, and most of them just organized it as One Big Table (OBT) or preprocessed the data types of frequently used tables and called it a data warehouse.</p><p>Our company was no exception. Without a proper data warehouse, dashboards and metrics were calculated differently by per people, leading to inconsistent results across products. Meanwhile, technical debt kept piling up.</p><p>In such an environment, constantly maintaining systems felt like <code>pouring water into a bottomless pit</code>.</p><p><img src="/en/images/5.png" alt="WHAT I DO"></p><h2 id="Why-Do-We-Need-a-Data-Warehouse"><a href="#Why-Do-We-Need-a-Data-Warehouse" class="headerlink" title="Why Do We Need a Data Warehouse?"></a>Why Do We Need a Data Warehouse?</h2><p>The absence of a data warehouse leads to the following major issues:</p><ol><li><p>Data quality management becomes challenging.</p><ul><li>Developers apply different metric definitions, causing confusion.</li><li>Changes in backend structures are often not communicated to the data team, leading to outdated data being used.</li><li>When metric logic changes, it’s hard to identify all the areas that need updates.</li></ul></li><li><p>Managing pipeline code becomes difficult.</p><ul><li>With everyone creating their own pipelines, maintenance becomes impossible if someone leaves the company.</li><li>Even within the data team, it’s unclear what products exist.</li><li>Maintenance consumes excessive time and resources.</li></ul></li></ol><p>A data warehouse plays a central role in managing data quality and maintaining efficient code. For example, when defining GMV for our company, some people calculated it using <code>pay_time</code>, while others used <code>create_time</code>. Such inconsistencies erode trust and reliability in the data. By defining a standard GMV formula and adding a GMV column to a warehouse table, we ensured all products pulled the value from the same source. Later, if the GMV calculation formula changes, updating the warehouse pipeline alone ensures consistency across all metrics.</p><p>Additionally, when backend logic changes or server issues cause data integrity problems, the warehouse manager can quickly address them. Without this centralization, different data products with independent pipelines create an exponential increase in maintenance tasks, rendering the system unmanageable. Over time, this results in significant legacy systems—akin to museum relics—that exist solely for troubleshooting. A data warehouse prevents such issues and provides an excellent solution.</p><p>Our company decided to address these challenges by building a proper data warehouse.</p><h2 id="How-to-Build-a-Data-Warehouse"><a href="#How-to-Build-a-Data-Warehouse" class="headerlink" title="How to Build a Data Warehouse"></a>How to Build a Data Warehouse</h2><p>So, how do you get started with building a data warehouse? The first step is to <code>understand the service domains</code>.<br>In our company, multiple domains exist within a single large service, each with distinct business logic. Understanding these business rules is essential to define the scope during data modeling.</p><p>The next step is to <code>understand the operational data</code> tied to the business logic.<br>Analytical data is fundamentally derived from service-generated data. Without understanding the service database logic, building a warehouse is impossible. This involves consulting with the backend team, reading historical documentation, and directly querying and exploring the schemas.</p><p>Once you’ve studied the data, you can move to the development phase.</p><p>The third step is to <code>design your data modeling</code>. This involves choosing between a star schema and a snowflake schema based on the business logic and data. For more details on these schema structures, check out <a href="https://dev-bearabbit.github.io/en/en/Engineering/engineering-0/">this post</a>.<br>Some tips to keep in mind during modeling:</p><ul><li>Define common metrics first and modeling around them.</li><li>Use logical primary keys for table integration.</li><li>Add data integrity labels (e.g., VALID, BAD_DATA, INVALID).</li><li>Document your data model thoroughly.</li><li>Include separate columns for INSERT and UPDATE timestamps.</li></ul><p>Once you’re done modeling your data, the next step is to <code>design your ETL/ELT pipeline</code>.<br>The pipeline should reflect the modeling results, supporting either INSERT or UPDATE operations. Choosing a platform that supports UPSERT is ideal. Our team uses Spark with the open-source Delta Lake library to implement UPSERT operations. Additionally, we set up Airflow to run daily batches, ensuring that the latest data is regularly updated or added. Creating pipeline templates can save time when configuring DAGs repeatedly.</p><p>Next, you need to configure an environment to <code>organize and share the table schemas in your warehouse</code>.<br>Organizing and sharing table schemas within the warehouse is essential. Usually, a data catalog platform is used for this purpose. Our team uses DataHub to manage and share schemas. Without documenting tables during modeling, you’ll face a mountain of overdue documentation tasks later.</p><p>The final step is to <code>organize your quality control and testing system</code>.<br>The warehouse assumes its data is high-quality by default, so maintaining quality control is crucial. Engineers responsible for the warehouse should continually think about quality and testing. This includes:</p><ul><li>Data integrity tests</li><li>NULL value checks</li><li>Duplicate data removal</li><li>Monitoring historical changes and outliers</li></ul><p>In reality, business logic and operational data logic constantly evolve as services grow and change.<br>Ultimately, data quality management involves recognizing, understanding, and consistently reflecting these changes.<br>For this reason, building and maintaining a data warehouse requires significant resources.<br>But if you build a product without these filters in place, you’re more likely to become a mass-production shop that accumulates technical debt that’s hard to reverse at scale.</p><p>To summarize, the process can be broken down as follows:</p><ol><li><p>Learning Phase</p><ul><li>Understand the business logic for each service domain.</li><li>Understand the operational data and database logic for the service.</li></ul></li><li><p>Development Phase</p><ul><li>Design the data model.</li><li>Design and develop the ETL&#x2F;ELT pipelines.</li><li>Build a data catalog.</li><li>Establish quality control and testing systems.</li></ul></li></ol><h2 id="Conclusion-Please-Build-a-Data-Warehouse"><a href="#Conclusion-Please-Build-a-Data-Warehouse" class="headerlink" title="Conclusion: Please Build a Data Warehouse"></a>Conclusion: Please Build a Data Warehouse</h2><p>A data warehouse forms the backbone of a data team, ensuring data quality and simplifying code maintenance. Addressing the aforementioned issues and goals can significantly enhance a data team’s productivity and efficiency.</p><p>A data warehouse is not just a storage solution but the heart of the data team, systematically managing data and providing reliable information. If infrastructure like this isn’t built when the system is manageable, the team will likely descend into maintenance hell.</p><p>So, the conclusion of this post is simple: please build a data warehouse early to prevent chaos down the road.</p>]]></content>
    
    
    <categories>
      
      <category>BigData</category>
      
      <category>engineering</category>
      
    </categories>
    
    
    <tags>
      
      <tag>BigData</tag>
      
      <tag>DataWarehouse</tag>
      
      <tag>DW</tag>
      
      <tag>DataQuality</tag>
      
      <tag>Pipeline</tag>
      
      <tag>DataEngineering</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Avoid a Cost Explosion with AWS Personalize</title>
    <link href="/en/en/Lesson/lesson-0/"/>
    <url>/en/en/Lesson/lesson-0/</url>
    
    <content type="html"><![CDATA[<p>In this post, I’ll share how I unexpectedly faced a cost explosion while using AWS Personalize. Hopefully, my experience can help others avoid similar pitfalls in SaaS cost management.</p><span id="more"></span><hr><h2 id="The-Incident"><a href="#The-Incident" class="headerlink" title="The Incident"></a>The Incident</h2><p>I was developing and testing a tool for extracting CRM campaign audiences. As part of this process, I compared AWS Personalize pricing, estimated the expected costs, and documented everything in a Confluence file to present to my lead. After approval, we moved forward with A&#x2F;B testing between our current method and the model output. For context, I used the <code>aws-item-affinity</code> recipe for item-user segmentation.</p><p>However, this morning, I received an alert about a sudden surge in Personalize costs. According to my calculations, the expenses should have been minimal. What went wrong? My spine chilled as I felt something was terribly off. Feeling a wave of panic, I immediately began investigating.</p><p><img src="/en/images/0.png" alt="What’s going on"></p><p>I spent the entire morning staring at the <a href="https://aws.amazon.com/personalize/pricing/">AWS Personalize pricing page</a> and recalculating costs based on usage. Finally, I identified the issue.</p><h2 id="Root-Cause"><a href="#Root-Cause" class="headerlink" title="Root Cause"></a>Root Cause</h2><p>The culprit was the batch segment job. While the dataset and training costs were minimal since they involved one-time charges, the batch segment cost was the bombshell. Let’s take a look at the pricing details from AWS’s official page.</p><p><img src="/en/images/1.png" alt="Pricing table for batch segment"></p><p>The table above outlines how batch segment costs are calculated. Here’s where I misunderstood: I assumed the term Users in dataset referred to the number of users requested per segment. That is, for up to 100,000 requests, the cost would be <code>$0.016</code> per segment request, and for 100,001–900,000 requests, the cost would drop to <code>$0.008</code>. Based on this, I calculated the cost for retrieving 10,000 users for each of 10 items like this:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-number">10000</span>/<span class="hljs-number">1000</span> * <span class="hljs-number">0.016</span> * <span class="hljs-number">10</span><br></code></pre></td></tr></table></figure><p>However, <code>Users in dataset</code> actually refers to <code>Total Users in the Dataset</code>—the total number of users in the training dataset—irrespective of the number of users requested in the segment job. Since I trained the model on a year’s worth of data, the dataset contained approximately 600,000 users. The correct calculation, then, is:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">Tier_1 = <span class="hljs-number">100000</span>/<span class="hljs-number">1000</span> * <span class="hljs-number">0.016</span> * <span class="hljs-number">10</span><br>Tier_2 = <span class="hljs-number">500000</span>/<span class="hljs-number">1000</span> * <span class="hljs-number">0.008</span> * <span class="hljs-number">10</span><br></code></pre></td></tr></table></figure><p>In my initial understanding, the cost would have been just <code>$1.6</code>, but the actual cost came to <code>$56</code>. This was entirely my mistake for not double-checking the details. Looking back, I should have questioned why the costs seemed so low and revisited the documentation for clarity. At the same time, I can’t help but feel like AWS’s pricing table is designed to be confusing—it never crossed my mind that the requested number of users wouldn’t factor into the calculation at all.</p><p><img src="/en/images/2.png" alt="Plz undo everything"></p><p>If you miscalculate the cost of using SaaS in the cloud ecosystem, you’ll end up paying a lot of unnecessary money, even if your app works.<br>I wrote my first postmortem documentation as a developer because of this cost. I felt sad because I made a mistake I didn’t expect.<br>For anyone confused like me when calculating costs, I’m leaving this on my blog.</p><h2 id="Lessons-Learned"><a href="#Lessons-Learned" class="headerlink" title="Lessons Learned"></a>Lessons Learned</h2><ul><li>Cloud costs are seriously scary. Sometimes, I wonder if server crashes would be stressful… Actually, no—that’s a nightmare too.</li><li>Always read AWS documentation carefully and double-check with others to avoid misunderstandings.</li><li>For AWS Personalize, pay close attention to segment costs. The size of the training dataset directly affects batch segment charges.</li></ul>]]></content>
    
    
    <categories>
      
      <category>etc.</category>
      
      <category>lessons</category>
      
    </categories>
    
    
    <tags>
      
      <tag>aws</tag>
      
      <tag>personalize</tag>
      
      <tag>pricing</tag>
      
      <tag>mistake</tag>
      
      <tag>aws-item-affinity</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Customizing Hexo for a Multilingual Blog</title>
    <link href="/en/en/AboutHexo/about-hexo-0/"/>
    <url>/en/en/AboutHexo/about-hexo-0/</url>
    
    <content type="html"><![CDATA[<p>Here’s how to customize your Hexo blog to support multiple languages.</p><span id="more"></span><hr><h2 id="Current-Status"><a href="#Current-Status" class="headerlink" title="Current Status"></a>Current Status</h2><p>Here’s the current state of my technical blog:</p><ul><li>Hexo with the Fluid theme is being used.</li><li>The Fluid theme does not natively support multiple languages.</li><li>Custom development is needed to enable multilingual functionality.</li></ul><h2 id="Customization-Requirements"><a href="#Customization-Requirements" class="headerlink" title="Customization Requirements"></a>Customization Requirements</h2><p>The customization requirements were as follows:</p><ul><li>The root URL should serve the Korean version, and the English version should be redirected to the &#x2F;en&#x2F; path.</li><li>Posts should be written independently for each language, with categories and tags tailored to the specific language.</li><li>A language selection dropdown should be added to the navbar, allowing users to switch languages.</li></ul><h2 id="Customization-Setup"><a href="#Customization-Setup" class="headerlink" title="Customization Setup"></a>Customization Setup</h2><p>The customization process was straightforward: create <strong>two separate Hexo blogs</strong> using the same theme and modify only the navbar.<br>To meet the requirements, the blog deployment and source code folders were structured as follows:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">drwxr-xr-x  18 ryu  staff   576  1  2 06:33 deploy<br>drwxr-xr-x  13 ryu  staff   416  1  2 05:22 en-blog<br>drwxr-xr-x  11 ryu  staff   352  1  3 01:47 ko-blog<br></code></pre></td></tr></table></figure><ul><li><code>en-blog</code>: Source code for the English blog</li><li><code>ko-blog</code>: Source code for the Korean blog</li><li><code>deploy</code>: Final folder containing files ready for deployment</li></ul><p>If you’re starting from scratch, create the blog source folders using the following commands:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">hexo init ko-blog<br>hexo init en-blog<br></code></pre></td></tr></table></figure><p>Then navigate to each folder, download the desired theme, and set the default configuration separately.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># Korean version</span><br><span class="hljs-built_in">cd</span> ko-blog<br>npm install --save hexo-theme-fluid<br><br><span class="hljs-comment"># English version</span><br><span class="hljs-built_in">cd</span> en-blog<br>npm install --save hexo-theme-fluid<br></code></pre></td></tr></table></figure><p>Since I planned to use the Korean version as the root URL, I configured the <code>_config.yml</code> files as follows:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># Korean version: /ko-blog/_config.yml</span><br>language: ko<br>url: https://dev-bearabbit.github.io<br>root: /<br>permalink: :lang/:title/<br><br><span class="hljs-comment"># English version: /en-blog/_config.yml</span><br>language: en<br>root: /en/<br>permalink: :lang/:title/<br></code></pre></td></tr></table></figure><p>This configuration adds the lang parameter to the permalink, so it’s essential to include lang in each post. To simplify this, I modified the default post scaffolds for each blog:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># Korean version: /ko-blog/scaffolds/post.md</span><br>---<br>title: &#123;&#123; title &#125;&#125;<br>lang: ko<br><span class="hljs-built_in">date</span>: &#123;&#123; <span class="hljs-built_in">date</span> &#125;&#125;<br>tags:<br>categories:<br>---<br><br><span class="hljs-comment"># English version: /en-blog/scaffolds/post.md</span><br>---<br>title: &#123;&#123; title &#125;&#125;<br>lang: en<br><span class="hljs-built_in">date</span>: &#123;&#123; <span class="hljs-built_in">date</span> &#125;&#125;<br>tags:<br>categories:<br>---<br></code></pre></td></tr></table></figure><p>Next, I added a language selection dropdown to the navbar by editing the theme’s navigation files. In the Fluid theme, this can be done in <code>layout/_partials/header/navigation.ejs</code>.</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs html"><span class="hljs-comment">&lt;!-- Korean version: /ko-blog/themes/fluid/layout/_partials/header/navigation.ejs --&gt;</span><br><span class="hljs-comment">&lt;!-- English version: /en-blog/themes/fluid/layout/_partials/header/navigation.ejs --&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">div</span> <span class="hljs-attr">class</span>=<span class="hljs-string">&quot;dropdown-menu&quot;</span> <span class="hljs-attr">aria-labelledby</span>=<span class="hljs-string">&quot;languageDropdown&quot;</span>&gt;</span><br>  <span class="hljs-tag">&lt;<span class="hljs-name">a</span> <span class="hljs-attr">class</span>=<span class="hljs-string">&quot;dropdown-item&quot;</span> <span class="hljs-attr">href</span>=<span class="hljs-string">&quot;/&quot;</span>&gt;</span>한국어<span class="hljs-tag">&lt;/<span class="hljs-name">a</span>&gt;</span><br>  <span class="hljs-tag">&lt;<span class="hljs-name">a</span> <span class="hljs-attr">class</span>=<span class="hljs-string">&quot;dropdown-item&quot;</span> <span class="hljs-attr">href</span>=<span class="hljs-string">&quot;/en/&quot;</span>&gt;</span>English<span class="hljs-tag">&lt;/<span class="hljs-name">a</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">div</span>&gt;</span><br></code></pre></td></tr></table></figure><p>To display the selected language in the navbar, I added the following script:</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs html"><span class="hljs-tag">&lt;<span class="hljs-name">script</span>&gt;</span><span class="language-javascript"></span><br><span class="language-javascript">  <span class="hljs-variable language_">document</span>.<span class="hljs-title function_">addEventListener</span>(<span class="hljs-string">&#x27;DOMContentLoaded&#x27;</span>, <span class="hljs-function">() =&gt;</span> &#123;</span><br><span class="language-javascript">    <span class="hljs-keyword">const</span> path = <span class="hljs-variable language_">window</span>.<span class="hljs-property">location</span>.<span class="hljs-property">pathname</span>;</span><br><span class="language-javascript">    <span class="hljs-keyword">const</span> currentLang = path.<span class="hljs-title function_">startsWith</span>(<span class="hljs-string">&#x27;/en/&#x27;</span>) ? <span class="hljs-string">&#x27;English&#x27;</span> : <span class="hljs-string">&#x27;한국어&#x27;</span>;</span><br><span class="language-javascript">    <span class="hljs-keyword">const</span> currentLangElement = <span class="hljs-variable language_">document</span>.<span class="hljs-title function_">getElementById</span>(<span class="hljs-string">&#x27;current-lang&#x27;</span>);</span><br><span class="language-javascript">    currentLangElement.<span class="hljs-property">textContent</span> = currentLang;</span><br><span class="language-javascript">  &#125;);</span><br><span class="language-javascript"></span><span class="hljs-tag">&lt;/<span class="hljs-name">script</span>&gt;</span><br></code></pre></td></tr></table></figure><p>Finally, I updated the <code>head.ejs</code> file to include alternate language links:</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs html"><span class="hljs-comment">&lt;!-- Korean version: /ko-blog/themes/fluid/layout/_partials/head.ejs --&gt;</span><br><span class="hljs-comment">&lt;!-- English version: /en-blog/themes/fluid/layout/_partials/head.ejs --&gt;</span><br><br><span class="hljs-tag">&lt;<span class="hljs-name">link</span> <span class="hljs-attr">rel</span>=<span class="hljs-string">&quot;alternate&quot;</span> <span class="hljs-attr">hreflang</span>=<span class="hljs-string">&quot;ko&quot;</span> <span class="hljs-attr">href</span>=<span class="hljs-string">&quot;https://dev-bearabbit.github.io/&quot;</span> /&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">link</span> <span class="hljs-attr">rel</span>=<span class="hljs-string">&quot;alternate&quot;</span> <span class="hljs-attr">hreflang</span>=<span class="hljs-string">&quot;en&quot;</span> <span class="hljs-attr">href</span>=<span class="hljs-string">&quot;https://dev-bearabbit.github.io/en/&quot;</span> /&gt;</span><br></code></pre></td></tr></table></figure><p>With these changes, the customization is complete.</p><h2 id="Deploying-the-Blog"><a href="#Deploying-the-Blog" class="headerlink" title="Deploying the Blog"></a>Deploying the Blog</h2><p>The blog is deployed using GitHub Pages. First, generate the deployment files for both blogs:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># Korean version: ko-blog</span><br><span class="hljs-comment"># English version: en-blog</span><br>hexo clean<br>hexo generate<br></code></pre></td></tr></table></figure><p>The generated files will be located in the following directories:</p><ul><li>Korean version: ko-blog&#x2F;public&#x2F;</li><li>English version: en-blog&#x2F;public&#x2F;</li></ul><p>Move the files into the <code>deploy</code> folder. Ensure the Korean version is moved to the root of <code>deploy</code>, while the English version is moved to <code>deploy/en/</code>.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">rm</span> -rf deploy/*<br><br><span class="hljs-built_in">mv</span> ko-blog/public/* deploy/<br><span class="hljs-built_in">mkdir</span> -p deploy/en/ &amp;&amp; <span class="hljs-built_in">mv</span> en-blog/public/* deploy/en/<br></code></pre></td></tr></table></figure><p>Finally, deploy the contents of the deploy folder to GitHub.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># Initial setup</span><br><span class="hljs-built_in">cd</span> deploy<br>git init<br>git remote add origin https://github.com/&lt;username&gt;/&lt;repository&gt;.git<br><br><span class="hljs-comment"># Push changes</span><br>git add .<br>git commit -m <span class="hljs-string">&quot;Deploy folder initial commit&quot;</span><br>git push origin main<br></code></pre></td></tr></table></figure><p>This completes the setup for a multilingual Hexo blog.</p>]]></content>
    
    
    <categories>
      
      <category>etc.</category>
      
      <category>about hexo</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Hexo</tag>
      
      <tag>blog</tag>
      
      <tag>multilingual</tag>
      
      <tag>custom</tag>
      
      <tag>English blog</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Basic Structure of Analytical Tables</title>
    <link href="/en/en/Engineering/engineering-0/"/>
    <url>/en/en/Engineering/engineering-0/</url>
    
    <content type="html"><![CDATA[<p>Before I begin, I’d like to note that this article is summarized and organized from Keisuke Nishida’s book Big Data Technology.</p><span id="more"></span><h2 id="Normalization-and-Denormalization"><a href="#Normalization-and-Denormalization" class="headerlink" title="Normalization and Denormalization"></a>Normalization and Denormalization</h2><p>Typically, relational databases(RDBs) are structured using normalized data, which is generally divided into transaction data and master data. <code>Transaction data</code> refers to data that is recorded sequentially over time, while <code>Master data</code> contains various types of reference information used by transaction data. Transaction data is immutable once recorded, whereas master data may change depending on circumstances.</p><p>The process of organizing data to minimize redundancy is called <code>Normalization</code>. By normalizing data, integrity can be maintained, and the storage size can be optimized.</p><p>However, tables used for analysis often undergo the opposite process called <code>Denormalization</code>. Denormalization involves combining transaction data and master data to make analysis more convenient. To create dimension tables for analysis, normalized tables are recombined into a single table. In this process, data redundancy is acceptable.</p><h2 id="Multidimensional-Model"><a href="#Multidimensional-Model" class="headerlink" title="Multidimensional Model"></a>Multidimensional Model</h2><p>A multidimensional model, as the name suggests, represents data across multiple dimensions. Normally, when we think of data, we envision a two-dimensional model consisting of rows and columns. However, operational data often has much more complex structures, making it challenging to represent it simply in two dimensions. Attempting to do so could result in significant data duplication and an unmanageable number of columns.</p><p>In industry terms, a multidimensional model refers to a design where data is divided into <code>fact tables</code>, which represent actual data, and <code>dimension tables</code>, which provide reference values. This structure minimizes redundancy and helps users better understand the scope of the data.</p><p>Multidimensional models are typically designed based on the metrics to be analyzed. On the other hand, MPP(Massively Parallel Processing) databases do not incorporate the concept of multidimensional models, instead relying on denormalized tables.</p><h2 id="Fact-Table-and-Dimension-Table"><a href="#Fact-Table-and-Dimension-Table" class="headerlink" title="Fact Table and Dimension Table"></a>Fact Table and Dimension Table</h2><p><code>Fact tables</code> record transactional data or observations, while <code>Dimension tables</code> store the reference data used by the fact tables. For example, numerical data used for aggregation is stored in fact tables, and dimension tables are used to classify this data by providing attributes.</p><p>As data volume grows, fact tables become significantly larger than dimension tables, and the size of fact tables directly affects aggregation performance. Therefore, keeping fact tables as small as possible is crucial for high performance. Only keys like IDs are retained in fact tables, with other attributes moved to dimension tables.</p><h2 id="Star-Schema-and-Snowflake-Schema"><a href="#Star-Schema-and-Snowflake-Schema" class="headerlink" title="Star Schema and Snowflake Schema"></a>Star Schema and Snowflake Schema</h2><p>The <code>Star Schema</code> organizes data with a fact table at the center, surrounded by multiple dimension tables. Dimension tables are single-layered and do not have hierarchical structures. The following image illustrates the structure of a star schema:</p><p><img src="/en/images/3.png" alt="star schema"></p><p>The <code>Snowflake Schema</code> extends the star schema by allowing dimension tables to have hierarchical sub-dimensions. In other words, it applies stronger normalization compared to the star schema. The following image shows the structure of a snowflake schema:</p><p><img src="/en/images/4.png" alt="snowflake schmea"></p><p>The comparison between the star schema and the snowflake schema is as follows:</p><table><thead><tr><th>Aspect</th><th>Star Schema</th><th>Snowflake Schema</th></tr></thead><tbody><tr><td>Advantages</td><td>- Simple data structure and queries. <br></td><td>- Minimizes data redundancy and storage space. <br> - Easier to maintain due to the absence of redundancy.</td></tr><tr><td>Disadvantages</td><td>- Dimension tables may have redundancy. <br> - Requires more storage space.</td><td>- Requires more joins, leading to potential performance issues. <br> - More complex data structure.</td></tr></tbody></table><p>Generally, the snowflake schema is more suitable for data warehouses, while the star schema is considered better for data marts.</p><h2 id="Columnar-Storage-and-Denormalized-Tables"><a href="#Columnar-Storage-and-Denormalized-Tables" class="headerlink" title="Columnar Storage and Denormalized Tables"></a>Columnar Storage and Denormalized Tables</h2><p>With the advent of columnar storage systems, the situation has changed. Even if the number of columns increases, the performance of query engines is not significantly affected, and column-level compression reduces disk I&#x2F;O overhead. Consequently, there is less need to separate data into dimension tables, and a single, large fact table may suffice.</p><p>A <code>Denormalized table</code> is created by further denormalizing a star schema and combining all the tables into one fact table. For data warehouses, the star schema remains optimal for table structures. During the data accumulation phase, separating fact and dimension tables is preferable, but when creating data marts, combining them into a denormalized table is often recommended.</p>]]></content>
    
    
    <categories>
      
      <category>BigData</category>
      
      <category>engineering</category>
      
    </categories>
    
    
    <tags>
      
      <tag>BigData</tag>
      
      <tag>DataWarehouse</tag>
      
      <tag>DataMart</tag>
      
      <tag>Schema</tag>
      
      <tag>StarSchema</tag>
      
      <tag>SnowflakeSchema</tag>
      
      <tag>DenormalizedTable</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
