<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>Relationship Safety Also Needs a Checkup</title>
    <link href="/en/en/FixGround/fixground-0/"/>
    <url>/en/en/FixGround/fixground-0/</url>
    
    <content type="html"><![CDATA[<p>To prevent repeated dating violence and relationship-related tragedies,<br>I built a simple test that helps anyone check for subtle warning signs in their romantic relationships.</p><span id="more"></span><hr><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>These days, words like dating violence, relationship-related homicides, and emotional abuse have sadly become all too familiar in the news.<br>Whenever I see another headline, I find myself wondering:</p><blockquote><p>“Why do these tragedies keep happening?”<br>“Wasn’t there some way to notice the warning signs sooner?”</p></blockquote><p>I carried those questions with me for a long time.<br>And one day, as the first experiment for <code>Fixground</code>, I decided to try answering them—<br>by making a simple test that scores your romantic relationship.</p><h2 id="Why-a-Test"><a href="#Why-a-Test" class="headerlink" title="Why a Test?"></a>Why a Test?</h2><p>Most experts agree that relationship violence almost never starts with sudden, extreme behavior.<br>In many cases, small and subtle warning signs begin to appear early on.</p><ul><li>Trouble managing emotions</li><li>Excessive possessiveness</li><li>Demanding constant contact</li><li>Subtle but repeated belittling</li></ul><p>These behaviors often get brushed aside, disappear for a while, or are excused as just part of being in a relationship.</p><blockquote><p>According to a survey by the Korea Women’s Hotline, 38.9% of respondents said they “did not recognize the behavior as violence” right after experiencing controlling behavior. 32.1% said, “I thought it was because they loved me.” After experiencing sexual violence, 29.3% said, “I didn’t think of it as violence,” while 28.9% said, “I felt ashamed.”</p></blockquote><p>The deeper the relationship grows, the more your partner knows your routines and emotions—making it even harder to notice something is off, let alone leave.<br>As a result, the cycle continues, often escalating to isolation and repeated violence, until it’s nearly impossible to get out.</p><p>I kept thinking: if only there were a simple tool that helped people objectively reflect whenever something felt “off.”<br>That’s why I created the <strong>Relationship Safety Test</strong>—a self-assessment tool for moments like these.</p><h2 id="How-I-Made-It"><a href="#How-I-Made-It" class="headerlink" title="How I Made It"></a>How I Made It</h2><p>I wanted to keep this test simple, lightweight, and accessible to anyone.<br>No sign-up, no names—just swipe through the questions and get a score.</p><p>But it’s not just for fun.<br>Each question is weighted based on the severity of the warning sign it addresses, and “killer questions” are assigned fixed high points to make sure serious red flags are always highlighted.<br>Instead of a basic yes&#x2F;no checklist, the goal was to give a more nuanced, relative risk score.<br>You can find the detailed scoring method in the <a href="https://github.com/dev-bearabbit/relationship-safety-test">README page</a>.</p><p>On the technical side, it’s built simply:</p><ul><li>Static site using GitHub Pages</li><li>Supports both Korean and English</li><li>Mobile-friendly, card-style UI</li></ul><p>The hope is that, even though it’s simple, this test can serve as an early warning for someone on the edge.</p><h2 id="What-I-Hope-For"><a href="#What-I-Hope-For" class="headerlink" title="What I Hope For"></a>What I Hope For</h2><p>This test is not perfect.<br>It can’t explain every situation or define every relationship.<br>But I hope that, for someone, it provides even a small sense of clarity.</p><blockquote><p>“Maybe I wasn’t just being sensitive—maybe it really was a problem.”<br>“I thought this was normal in a relationship, but it wasn’t.”</p></blockquote><p>I hope you stop doubting yourself.<br>Even if you never explain it to anyone else,<br>I hope this helps you check in with yourself, just once.</p><p>And I hope no one ever has to become another silent victim.<br>Before small discomfort grows into real tragedy,<br>let this test be a checkpoint for someone to reconsider their relationship.</p><p>If you want to try it yourself: <a href="https://dev-bearabbit.github.io/relationship-safety-test/">Go to the Relationship Safety Test</a></p>]]></content>
    
    
    <categories>
      
      <category>SeriesHub</category>
      
      <category>fixground notes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>experiment</tag>
      
      <tag>self-assessment</tag>
      
      <tag>relationship</tag>
      
      <tag>dating-violence</tag>
      
      <tag>test</tag>
      
      <tag>social-issue</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Terraform, How a Small Data Team Survived</title>
    <link href="/en/en/Terraform/terraform-0/"/>
    <url>/en/en/Terraform/terraform-0/</url>
    
    <content type="html"><![CDATA[<p>This post shares the lessons learned and structural decisions we made while adopting Terraform in a small data team. We also highlight key choices that helped us reduce errors and improve maintainability using Terraform Cloud.</p><span id="more"></span><h2 id="Why-We-Adopted-Terraform"><a href="#Why-We-Adopted-Terraform" class="headerlink" title="Why We Adopted Terraform"></a>Why We Adopted Terraform</h2><p>Terraform is commonly associated with SRE or DevOps teams, but it’s just as relevant for data teams—especially those responsible for building and maintaining diverse infrastructure. Our case was no different, except for one twist: just two of us were managing the entire stack.</p><p>Previously, we managed everything through the AWS web console, which led to several challenges:</p><ul><li>Difficult handoff when team members changed</li><li>Hard to track which service used which resource</li><li>No clear documentation of how infrastructure was built</li><li>Troublesome to reproduce or recover during outages</li><li>No easy way to distinguish active vs unused resources</li><li>Poor visibility into overall structure</li></ul><p>To address these issues, we decided to adopt Terraform in 2023.</p><h2 id="Early-Pain-Points"><a href="#Early-Pain-Points" class="headerlink" title="Early Pain Points"></a>Early Pain Points</h2><p>Over the course of 2024, we migrated and maintained large infrastructure components—load balancers, deployment servers, Spark clusters—using Terraform. Along the way, we made some important realizations:</p><ul><li>Grouping shared resources isn’t always ideal</li></ul><p>We initially grouped ALB, NLB, and other load balancers in a shared elb directory. However, even minor changes would trigger plans across all resources, making it hard to isolate changes. This introduced operational risks. We learned that organizing <code>code by service</code> rather than by resource type was safer and more practical for small teams.</p><ul><li>Over-abstracted modules became rigid and bloated</li></ul><p>In our effort to maximize <code>reusability</code>, we built composite modules like ec2_bundle that provisioned EC2 instances, EBS volumes, and IAM roles together. Later, when we needed to use only EC2 in another environment, the module was unusable. What began as a DRY solution turned into a bottleneck. We concluded that starting with <code>minimal functional modules</code> and composing them externally gave us more flexibility and easier maintenance.</p><ul><li>Too much abstraction reduced clarity and predictability</li></ul><p>Using for_each, merge, and dynamic made our code shorter and DRY—but it also introduced more bugs, such as type mismatches and null references from conditional logic. Despite looking neat, the code became a black box with frequent plan failures. We found that in a small team, it was safer to be explicit and repetitive than clever.</p><p>Terraform is a tool for humans.<br>Code that humans don’t understand quickly becomes a <code>failure</code> and another <code>maintenance</code> target.</p><h2 id="Our-Core-Design-Priorities"><a href="#Our-Core-Design-Priorities" class="headerlink" title="Our Core Design Priorities"></a>Our Core Design Priorities</h2><p>After identifying those pitfalls, we set three key priorities for our Terraform codebase:</p><ul><li>Clarity and usability: anyone should be able to understand the code at a glance.</li><li>Maintainability: infrastructure should be easy to troubleshoot and fix.</li><li>Terraform Cloud compatibility: environments should be isolated and run without conflicts.</li></ul><p>Ultimately, Terraform is about <code>people</code> managing <code>infrastructure</code> through <code>code</code>. Instead of obsessing over <code>DRYness</code> or <code>automation</code>, we learned to prioritize <code>stability</code>.</p><h2 id="Key-Architectural-Decisions"><a href="#Key-Architectural-Decisions" class="headerlink" title="Key Architectural Decisions"></a>Key Architectural Decisions</h2><p>Because we were a small team, we had to optimize for clarity and reduce room for error. Here are the main decisions we made, with options considered and why we chose them.</p><h3 id="Environment-Separation-Strategy"><a href="#Environment-Separation-Strategy" class="headerlink" title="Environment Separation Strategy"></a>Environment Separation Strategy</h3><ul><li>Problem: How should we separate dev and prod environments?</li><li>Options:<ul><li>Use .tfvars with shared directories</li><li>Use Git branches to separate environments</li><li>Use separate dev&#x2F; and prod&#x2F; folders</li></ul></li><li>Chosen: Folder-based separation</li><li>Reason: .tfvars and branch-based setups lacked clarity and were error-prone in Terraform Cloud. Folder-based structures improved readability and reduced mistakes.</li></ul><h3 id="Directory-Structure-Design"><a href="#Directory-Structure-Design" class="headerlink" title="Directory Structure Design"></a>Directory Structure Design</h3><ul><li>Problem: How should we organize resources?</li><li>Options:<ul><li>Separate modules and environments by resource</li><li>Modules by resource, envs by block</li><li>Modules by resource, envs by service&#x2F;shared</li></ul></li><li>Chosen: <code>modules/</code> for resource-level modules, <code>service/</code> and <code>shared/</code> for environments</li><li>Reason: This allowed clear separation of service-specific and shared resources like ALB and S3. It also aligned well with our CI&#x2F;CD flow.</li></ul><h3 id="Module-Reuse-Strategy"><a href="#Module-Reuse-Strategy" class="headerlink" title="Module Reuse Strategy"></a>Module Reuse Strategy</h3><ul><li>Problem: Should we use composite modules?</li><li>Options:<ul><li>Composite modules that wrap multiple resources</li><li>Functional modules combined in context</li></ul></li><li>Chosen: Functional modules with composition left to the caller</li><li>Reason: Composite modules lacked flexibility and didn’t adapt well across use cases. Smaller modules were easier to reuse and maintain.</li></ul><h3 id="Naming-Conventions"><a href="#Naming-Conventions" class="headerlink" title="Naming Conventions"></a>Naming Conventions</h3><ul><li>Problem: How should we name resources?</li><li>Options:<ul><li>Freeform names with unified tags</li><li>Base conventions with flexible exceptions</li><li>Strict naming conventions without exceptions</li></ul></li><li>Chosen: Strict, consistent naming conventions</li><li>Reason: Clear naming made it easy to identify resources visually and helped with filtering in AWS Console, CloudWatch, and Billing. Tags alone weren’t enough.</li></ul><h3 id="Abstraction-Level"><a href="#Abstraction-Level" class="headerlink" title="Abstraction Level"></a>Abstraction Level</h3><ul><li>Problem: How much should we abstract repeated code?</li><li>Options:<ul><li>Use <code>for_each</code>, <code>dynamic</code>, etc. to avoid repetition</li><li>Write explicit, repetitive code for readability</li></ul></li><li>Chosen: Prefer explicit code with minimal repetition</li><li>Reason: While abstraction reduced lines of code, it introduced more plan errors and debugging friction. Explicit code was safer and more predictable.</li></ul><h3 id="Terraform-Cloud-Workflow"><a href="#Terraform-Cloud-Workflow" class="headerlink" title="Terraform Cloud Workflow"></a>Terraform Cloud Workflow</h3><ul><li>Problem: How can we control the scope of changes in automated workflows?</li><li>Options:<ul><li>One workspace for all resources</li><li>Separate workspace per service</li></ul></li><li>Chosen: One workspace per service</li><li>Reason: Service-level workspaces isolated change scope and made plans easier to review. Terraform Cloud’s automated plans per workspace helped prevent accidental changes.</li></ul><h2 id="Final-Thoughts-Why-These-Decisions-Mattered"><a href="#Final-Thoughts-Why-These-Decisions-Mattered" class="headerlink" title="Final Thoughts: Why These Decisions Mattered"></a>Final Thoughts: Why These Decisions Mattered</h2><p>To wrap up:</p><ul><li>Simplicity and clarity are critical for small teams.</li><li>Over-abstraction and DRY obsession often backfire in Terraform.</li><li>Folder-based environment separation worked better than branch-based setups.</li><li>Terraform Cloud is powerful, but only when used with a well-structured codebase.</li></ul><p>At the end of the day, the most important qualities in Terraform practice were <code>readability</code>, <code>clarity</code>, and <code>recoverability</code>.<br>Small teams don’t need the cleverest Terraform code.<br>They need the most understandable one.</p>]]></content>
    
    
    <categories>
      
      <category>DevOps</category>
      
      <category>terraform</category>
      
    </categories>
    
    
    <tags>
      
      <tag>terraform</tag>
      
      <tag>iac</tag>
      
      <tag>infra</tag>
      
      <tag>data engineering</tag>
      
      <tag>small team</tag>
      
      <tag>terraform cloud</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>ChatGPT as a Data Analyst with DuckDB + S3</title>
    <link href="/en/en/Experiment/experiment-0/"/>
    <url>/en/en/Experiment/experiment-0/</url>
    
    <content type="html"><![CDATA[<p>In this post, I’ll walk you through how I connected ChatGPT to my own dataset stored in S3 using DuckDB, and had the AI analyze it like a real data analyst.</p><span id="more"></span><hr><h2 id="Getting-started"><a href="#Getting-started" class="headerlink" title="Getting started"></a>Getting started</h2><p>I recently became interested in real estate data, so I loaded it into my personal S3 and started playing around with it.<br>While querying and analyzing the data myself was fun, I suddenly had a thought:</p><blockquote><p>“How much easier would it be to have AI analyze and answer my questions for me?”</p></blockquote><p>I realized that calling tools like Model Context Protocol (MCP) is a hot topic these days,  I wondered if it would be possible to create a structure where AI could directly query my data and analyze it.</p><p>However, OpenAI didn’t directly support <code>.context.yaml</code> or the MCP protocol. The recently released SDK only works with the Agents API, and the ChatGPT web interfaces (GPTs) don’t allow you to write MCP directly.<br>Instead, GPTs can automatically discover API servers that provide an <code>openapi.json</code> and register it as a function,<br>I figured I could mimic an “AI Data Analyst” that behaves like an MCP using this setup.</p><p>So I decided to give it a try.</p><h2 id="Architecture"><a href="#Architecture" class="headerlink" title="Architecture"></a>Architecture</h2><p>The structure we used in this experiment looks like this</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs text">[GPTs (web interface)]<br><br>    ↓ function-calling<br><br>[FastAPI server (DuckDB + S3 connection)]<br><br>    ↓ DuckDB query<br><br>[real estate parquet file stored in S3]<br></code></pre></td></tr></table></figure><ol><li>The user asks a question in natural language inside GPT.</li><li>GPT looks up the registered tool via <code>openapi.json</code>, selects the right API and builds the request.</li><li>The API call hits a FastAPI server running locally.</li><li>FastAPI connects to DuckDB and runs a SQL query on Parquet files stored in S3.</li><li>The result is returned to GPT, which summarizes it in natural language.</li><li>Additional analysis can be done based on the returned data.</li></ol><h2 id="Creating-an-API-Server-to-Query-S3"><a href="#Creating-an-API-Server-to-Query-S3" class="headerlink" title="Creating an API Server to Query S3"></a>Creating an API Server to Query S3</h2><p>I used <a href="https://data.seoul.go.kr/dataList/OA-21275/S/1/datasetView.do">Seoul Open Data Plaza</a> to get the real estate dataset and uploaded it to S3.</p><p>The data is stored in a structured path like this:</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs text">f&quot;seoul_apt_data/&#123;year&#125;/&#123;month:02d&#125;/&#123;day:02d&#125;.parquet&quot;<br></code></pre></td></tr></table></figure><p>This makes it easy to partition and query by date.</p><h3 id="Step-1-Install-Required-Packages"><a href="#Step-1-Install-Required-Packages" class="headerlink" title="Step 1. Install Required Packages"></a>Step 1. Install Required Packages</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">pip3 install fastapi uvicorn duckdb<br></code></pre></td></tr></table></figure><h3 id="Step-2-main-py-–-API-Server-Code"><a href="#Step-2-main-py-–-API-Server-Code" class="headerlink" title="Step 2. main.py – API Server Code"></a>Step 2. main.py – API Server Code</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> fastapi <span class="hljs-keyword">import</span> FastAPI, Query<br><span class="hljs-keyword">from</span> fastapi.staticfiles <span class="hljs-keyword">import</span> StaticFiles<br><span class="hljs-keyword">import</span> duckdb<br><span class="hljs-keyword">import</span> os<br><br>app = FastAPI()<br><br>S3_REGION = <span class="hljs-string">&quot;ap-northeast-2&quot;</span><br>S3_BUCKET = <span class="hljs-string">&quot;your-bucket-name&quot;</span><br>S3_ACCESS_KEY = os.getenv(<span class="hljs-string">&quot;AWS_ACCESS_KEY_ID&quot;</span>)<br>S3_SECRET_KEY = os.getenv(<span class="hljs-string">&quot;AWS_SECRET_ACCESS_KEY&quot;</span>)<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">duckdb_connect</span>():<br>    con = duckdb.connect()<br>    con.execute(<span class="hljs-string">f&quot;SET s3_region=&#x27;<span class="hljs-subst">&#123;S3_REGION&#125;</span>&#x27;;&quot;</span>)<br>    con.execute(<span class="hljs-string">f&quot;SET s3_access_key_id=&#x27;<span class="hljs-subst">&#123;S3_ACCESS_KEY&#125;</span>&#x27;;&quot;</span>)<br>    con.execute(<span class="hljs-string">f&quot;SET s3_secret_access_key=&#x27;<span class="hljs-subst">&#123;S3_SECRET_KEY&#125;</span>&#x27;;&quot;</span>)<br>    <span class="hljs-keyword">return</span> con<br><br><span class="hljs-meta">@app.get(<span class="hljs-params"><span class="hljs-string">&quot;/data_by_date&quot;</span></span>)</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_data_by_date</span>(<span class="hljs-params"></span><br><span class="hljs-params">    year: <span class="hljs-built_in">int</span> = Query(<span class="hljs-params">...</span>),</span><br><span class="hljs-params">    month: <span class="hljs-built_in">int</span> = Query(<span class="hljs-params">...</span>),</span><br><span class="hljs-params">    day: <span class="hljs-built_in">int</span> = Query(<span class="hljs-params">...</span>)</span><br><span class="hljs-params"></span>):<br>    con = duckdb_connect()<br>    path = <span class="hljs-string">f&quot;s3://<span class="hljs-subst">&#123;S3_BUCKET&#125;</span>/seoul_apt_data/<span class="hljs-subst">&#123;year&#125;</span>/<span class="hljs-subst">&#123;month:02d&#125;</span>/<span class="hljs-subst">&#123;day:02d&#125;</span>.parquet&quot;</span><br><br>    <span class="hljs-keyword">try</span>:<br>        df = con.execute(<span class="hljs-string">f&quot;SELECT CGG_NM, ARCH_AREA, ARCH_YR, THING_AMT FROM read_parquet(&#x27;<span class="hljs-subst">&#123;path&#125;</span>&#x27;)&quot;</span>).fetchdf()<br>        <span class="hljs-keyword">return</span> &#123;<br>            <span class="hljs-string">&quot;status&quot;</span>: <span class="hljs-string">&quot;success&quot;</span>,<br>            <span class="hljs-string">&quot;rows&quot;</span>: <span class="hljs-built_in">len</span>(df),<br>            <span class="hljs-string">&quot;data&quot;</span>: df.to_dict(orient=<span class="hljs-string">&quot;records&quot;</span>)<br>        &#125;<br>    <span class="hljs-keyword">except</span> Exception <span class="hljs-keyword">as</span> e:<br>        <span class="hljs-keyword">return</span> &#123;<br>            <span class="hljs-string">&quot;status&quot;</span>: <span class="hljs-string">&quot;error&quot;</span>,<br>            <span class="hljs-string">&quot;message&quot;</span>: <span class="hljs-built_in">str</span>(e)<br>        &#125;<br><br><span class="hljs-keyword">if</span> os.path.exists(<span class="hljs-string">&quot;.well-known&quot;</span>):<br>    app.mount(<span class="hljs-string">&quot;/.well-known&quot;</span>, StaticFiles(directory=<span class="hljs-string">&quot;.well-known&quot;</span>), name=<span class="hljs-string">&quot;well-known&quot;</span>)<br></code></pre></td></tr></table></figure><h3 id="Step-3-Run-the-Server"><a href="#Step-3-Run-the-Server" class="headerlink" title="Step 3. Run the Server"></a>Step 3. Run the Server</h3><p>We’re testing it out, so let’s run it locally for now.</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">uvicorn main:app --reload<br></code></pre></td></tr></table></figure><h3 id="Step-4-Test-the-API-Locally"><a href="#Step-4-Test-the-API-Locally" class="headerlink" title="Step 4. Test the API Locally"></a>Step 4. Test the API Locally</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> requests <span class="hljs-keyword">as</span> rq<br><br>params = &#123;<br>    <span class="hljs-string">&quot;year&quot;</span>: <span class="hljs-number">2024</span>,<br>    <span class="hljs-string">&quot;month&quot;</span>: <span class="hljs-number">12</span>,<br>    <span class="hljs-string">&quot;day&quot;</span>: <span class="hljs-number">1</span><br>&#125;<br><br>res = rq.get(<span class="hljs-string">&quot;http://127.0.0.1:8000/data_by_date&quot;</span>, params=params)<br></code></pre></td></tr></table></figure><p>If the response is returned successfully, your API server is ready.</p><h2 id="Registering-the-Tool-in-GPTs"><a href="#Registering-the-Tool-in-GPTs" class="headerlink" title="Registering the Tool in GPTs"></a>Registering the Tool in GPTs</h2><p>GPTs can auto-discover your tool via <code>openapi.json</code>. You’ll need:</p><ol><li>A public URL for your local server (we’ll use <code>ngrok</code>).</li><li>Two files under <code>.well-known/</code>: <code>ai-plugin.json</code>, <code>openapi.json</code></li></ol><h3 id="Open-Public-Access-with-ngrok"><a href="#Open-Public-Access-with-ngrok" class="headerlink" title="Open Public Access with ngrok"></a>Open Public Access with ngrok</h3><p>First, download <code>ngrok</code>. On Mac, you can install it with the following command<br>If you’re using another OS, you can download it from <a href="https://ngrok.com/downloads/mac-os">this site</a>.</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">brew install ngrok<br></code></pre></td></tr></table></figure><p>After installing, sign up on the <a href="https://dashboard.ngrok.com/signup">ngrok homepage</a> and get an <code>Auth Token</code> token to register as shown below.</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">ngrok config add-authtoken &lt;your-token&gt;<br></code></pre></td></tr></table></figure><p>Now expose local port 8000 to the outside world with the command below.</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">&gt; </span><span class="language-bash">ngrok http 8000</span><br></code></pre></td></tr></table></figure><p>Once the tunnel is open, you’ll see an address like this:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">Forwarding https://56a3-xxxx.ngrok-free.app -&gt; http://localhost:8000 <br></code></pre></td></tr></table></figure><h3 id="Setup-well-known"><a href="#Setup-well-known" class="headerlink" title="Setup .well-known/"></a>Setup <code>.well-known/</code></h3><p>Create a <code>.well-known</code> directory at the top of the FastAPI server and create the following two files.</p><ul><li>ai-plugin.json</li><li>openapi.json</li></ul><p>An example of <code>ai-plugin.json</code> is shown below.</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs json"><span class="hljs-punctuation">&#123;</span><br>  <span class="hljs-attr">&quot;schema_version&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;v1&quot;</span><span class="hljs-punctuation">,</span><br>  <span class="hljs-attr">&quot;name_for_human&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;Seoul Real Estate Data API&quot;</span><span class="hljs-punctuation">,</span><br>  <span class="hljs-attr">&quot;name_for_model&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;seoul_real_estate_data&quot;</span><span class="hljs-punctuation">,</span><br>  <span class="hljs-attr">&quot;description_for_human&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;An API for querying and analyzing apartment transaction data in Seoul.&quot;</span><span class="hljs-punctuation">,</span><br>  <span class="hljs-attr">&quot;description_for_model&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;Use this API to retrieve and analyze real estate transaction data in Seoul. Supports full or date-based queries.&quot;</span><span class="hljs-punctuation">,</span><br>  <span class="hljs-attr">&quot;auth&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">&#123;</span><br>    <span class="hljs-attr">&quot;type&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;none&quot;</span><br>  <span class="hljs-punctuation">&#125;</span><span class="hljs-punctuation">,</span><br>  <span class="hljs-attr">&quot;api&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">&#123;</span><br>    <span class="hljs-attr">&quot;type&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;openapi&quot;</span><span class="hljs-punctuation">,</span><br>    <span class="hljs-attr">&quot;url&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;https://56a3-xxxx.ngrok-free.app/.well-known/openapi.json&quot;</span><br>  <span class="hljs-punctuation">&#125;</span><span class="hljs-punctuation">,</span><br>  <span class="hljs-attr">&quot;contact_email&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;dev.bearabbit@gmail.com&quot;</span><br><span class="hljs-punctuation">&#125;</span><br></code></pre></td></tr></table></figure><p>Here we need to change the <code>url</code> value to the URL exposed earlier.</p><p>FastAPI automatically exposes the API specification in the path <code>/openapi.json</code>.<br>You can copy it to your <code>.well-known</code> directory.</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">curl http://127.0.0.1:8000/openapi.json -o .well-known/openapi.json<br></code></pre></td></tr></table></figure><p>In the <code>/openapi.json</code> file, you’ll need to enter additional <code>Servers</code> information, which you’ll add later when you connect the tool.</p><h2 id="Connecting-the-Tool-to-GPTs"><a href="#Connecting-the-Tool-to-GPTs" class="headerlink" title="Connecting the Tool to GPTs"></a>Connecting the Tool to GPTs</h2><p>Finally, you can create the tool from the GPT homepage.</p><p>First, click “Explore GPT &gt; Create” on <a href="https://chat.openai.com/gpts">this page</a>.<br>Next, enter the desired information as shown in the image below.</p><p><img src="/en/images/14.png" alt="update info"></p><p>Finally, add a task.</p><p><img src="/en/images/15.png" alt="add task"></p><p>Note that there is no authentication, just a schema.<br>The schema can be found using the URL <code>https://56a3-xxxx.ngrok-free.app/.well-known/openapi.json</code>.<br>The updated schema adds the following to the schema.</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs json"><span class="hljs-attr">&quot;servers&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">[</span><br>      <span class="hljs-punctuation">&#123;</span><br>      <span class="hljs-attr">&quot;url&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;https://bb1c-221-149-204-63.ngrok-free.app&quot;</span><br>      <span class="hljs-punctuation">&#125;</span><br>  <span class="hljs-punctuation">]</span><span class="hljs-punctuation">,</span><br></code></pre></td></tr></table></figure><p>This completes the connection.</p><h2 id="Test"><a href="#Test" class="headerlink" title="Test"></a>Test</h2><p>Now that you have your own analyst, it’s time to ask the questions you really want to know.</p><h3 id="What-were-the-real-estate-price-trends-last-week"><a href="#What-were-the-real-estate-price-trends-last-week" class="headerlink" title="What were the real estate price trends last week?"></a>What were the real estate price trends last week?</h3><p><img src="/en/images/16.png" alt="test 1"></p><h3 id="Is-apartment-pricing-declining-in-2025"><a href="#Is-apartment-pricing-declining-in-2025" class="headerlink" title="Is apartment pricing declining in 2025?"></a>Is apartment pricing declining in 2025?</h3><p><img src="/en/images/17.png" alt="test 2"></p><h3 id="Based-on-the-data-should-I-consider-buying-right-now"><a href="#Based-on-the-data-should-I-consider-buying-right-now" class="headerlink" title="Based on the data, should I consider buying right now?"></a>Based on the data, should I consider buying right now?</h3><p><img src="/en/images/18.png" alt="test 3_1"><br><img src="/en/images/19.png" alt="test 3_2"></p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>It was genuinely surprising how well GPT could pull and analyze my own data.<br>After building it myself, the pros and cons became pretty clear.</p><h3 id="What-worked-well"><a href="#What-worked-well" class="headerlink" title="What worked well"></a>What worked well</h3><ul><li>It really feels like having a junior data analyst that responds in natural language.</li><li>I now have someone to debate with about buying an apartment.</li><li>The interaction was intuitive once the tool was registered.</li></ul><h3 id="What-needs-improvement"><a href="#What-needs-improvement" class="headerlink" title="What needs improvement"></a>What needs improvement</h3><ul><li>Large datasets are hard to handle. GPTs have strict limits on function-call response sizes.</li><li>With the free version of ngrok, the public URL changes every time, which is annoying.</li></ul><p>The limitations are mostly solvable—with better API server design or a bit of investment.<br>This was just a small experiment, but it opens the door to building even smarter and more powerful tools in the future.</p>]]></content>
    
    
    <categories>
      
      <category>MachineLearning</category>
      
      <category>experiment</category>
      
    </categories>
    
    
    <tags>
      
      <tag>DeepLearning</tag>
      
      <tag>LLM</tag>
      
      <tag>ChatGPT</tag>
      
      <tag>openai</tag>
      
      <tag>FastAPI</tag>
      
      <tag>DuckDB</tag>
      
      <tag>S3</tag>
      
      <tag>AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Build Your Own Data Warehouse with DuckDB + S3</title>
    <link href="/en/en/Engineering/engineering-3/"/>
    <url>/en/en/Engineering/engineering-3/</url>
    
    <content type="html"><![CDATA[<p>Introducing an easy way to store data in AWS S3 and query it directly from your local machine using DuckDB.</p><span id="more"></span><h2 id="Why-DuckDB-S3"><a href="#Why-DuckDB-S3" class="headerlink" title="Why DuckDB + S3?"></a>Why DuckDB + S3?</h2><ul><li>DuckDB: Lightweight like SQLite, but with a columnar engine for speed and strong SQL support</li><li>S3: Store large datasets without worry and access them from anywhere, whether local or cloud</li><li>Simple Setup: No complex infrastructure needed—start analyzing data right away</li></ul><h2 id="Architecture-Task-Outline"><a href="#Architecture-Task-Outline" class="headerlink" title="Architecture &amp; Task Outline"></a>Architecture &amp; Task Outline</h2><p>Here’s how the structure looks:</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs text">[S3] Data Storage ──────▶ [DuckDB] Query locally<br>                         └──────▶ Metadata &amp; Schema Management (.duckdb file)<br></code></pre></td></tr></table></figure><p>Actual table data is stored in S3, while metadata and schema files are kept locally.</p><p>This way, you can analyze freely without worrying about storage limits.<br>Today’s task order:</p><ul><li>Install DuckDB on Mac</li><li>Learn how to use DuckDB UI</li><li>Connect DuckDB to S3</li><li>Apply .duckdbrc configuration</li><li>Test S3 tables from DuckDB UI</li></ul><h2 id="Installing-DuckDB-on-Mac"><a href="#Installing-DuckDB-on-Mac" class="headerlink" title="Installing DuckDB on Mac"></a>Installing DuckDB on Mac</h2><p>Installation is super simple via terminal:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">&gt; brew install duckdb<br></code></pre></td></tr></table></figure><p>Once installed, you can use it in <code>Transient In-Memory Mode</code> like below:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs bash">&gt; duckdb<br>v1.2.1 8e52ec4395<br>Enter <span class="hljs-string">&quot;.help&quot;</span> <span class="hljs-keyword">for</span> usage hints.<br>Connected to a transient in-memory database.<br>Use <span class="hljs-string">&quot;.open FILENAME&quot;</span> to reopen on a persistent database.<br>D <br></code></pre></td></tr></table></figure><p>DuckDB offers a REPL mode by default. Depending on whether you want one-time use or persistence, choose:</p><ul><li>Transient In-Memory DB: If you connect without <code>.open</code>, data is kept in memory and lost on exit</li><li>Persistent DB: <code>.duckdb</code> file stores schema&#x2F;data and can be reused</li></ul><h2 id="Using-DuckDB-UI"><a href="#Using-DuckDB-UI" class="headerlink" title="Using DuckDB UI"></a>Using DuckDB UI</h2><p>You can launch the UI, which is notebook-style, using a simple CLI command:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs shell">❯ duckdb -ui<br>┌──────────────────────────────────────┐<br>│                result                │<br>│               varchar                │<br>├──────────────────────────────────────┤<br>│ UI started at http://localhost:4213/ │<br>└──────────────────────────────────────┘<br>v1.2.1 8e52ec4395<br>Enter &quot;.help&quot; for usage hints.<br>D <br></code></pre></td></tr></table></figure><p>Visiting <code>http://localhost:4213/</code>, you’ll see a welcome page like this:</p><p><img src="/en/images/7.png" alt="duckdb page"></p><p>Click on <code>attached databases</code> at the top left and create a <code>.duckdb</code> file via the File option.</p><p><img src="/en/images/8.png" alt="add database"></p><p>You’ll see the <code>.duckdb</code> file created at the specified path:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs shell">~/Desktop/duckdb on  main [✘»!+?] on ☁️  (ap-northeast-2) <br>❯ ls -al<br>total 24<br>drwxr-xr-x   3 ryu  staff     96  3 23 20:53 .<br>drwx------@ 24 ryu  staff    768  3 23 20:53 ..<br>-rw-r--r--   1 ryu  staff  12288  3 23 20:53 mywarehouse.duckdb<br></code></pre></td></tr></table></figure><p>DuckDB UI allows three types of database connections:</p><ul><li>File: Connect or create <code>.duckdb</code> files in the local file system</li><li>URL: Useful for sharing read-only DB files in a centralized storage</li><li>Memory: Ideal for tests, one-off queries, or temporary sessions</li></ul><p>The URL option can be really handy in team environments.<br>Engineers can ETL and upload, while analysts&#x2F;managers can use the data.<br>Since it’s read-only, there’s no risk of accidentally deleting data.<br>However, whenever a new table is added or schema changes, someone needs to update the <code>.duckdb</code> file.</p><h2 id="Connecting-DuckDB-to-S3"><a href="#Connecting-DuckDB-to-S3" class="headerlink" title="Connecting DuckDB to S3"></a>Connecting DuckDB to S3</h2><p>First, create an S3 bucket to store your data. Disable all public access options.</p><p><img src="/en/images/9.png" alt="create S3 bucket"></p><p>Then go to <code>IAM &gt; Policies</code> and create a policy like this, allowing the specific bucket access:</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs json"><span class="hljs-punctuation">&#123;</span><br>    <span class="hljs-attr">&quot;Version&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;2012-10-17&quot;</span><span class="hljs-punctuation">,</span><br>    <span class="hljs-attr">&quot;Statement&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">[</span><br>        <span class="hljs-punctuation">&#123;</span><br>        <span class="hljs-attr">&quot;Effect&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;Allow&quot;</span><span class="hljs-punctuation">,</span><br>        <span class="hljs-attr">&quot;Action&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">[</span><br>            <span class="hljs-string">&quot;s3:GetObject&quot;</span><span class="hljs-punctuation">,</span><br>            <span class="hljs-string">&quot;s3:ListBucket&quot;</span><span class="hljs-punctuation">,</span><br>            <span class="hljs-string">&quot;s3:PutObject&quot;</span><span class="hljs-punctuation">,</span><br>            <span class="hljs-string">&quot;s3:DeleteObject&quot;</span><br>        <span class="hljs-punctuation">]</span><span class="hljs-punctuation">,</span><br>        <span class="hljs-attr">&quot;Resource&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">[</span><br>            <span class="hljs-string">&quot;arn:aws:s3:::your-bucket-name&quot;</span><span class="hljs-punctuation">,</span><br>            <span class="hljs-string">&quot;arn:aws:s3:::your-bucket-name/*&quot;</span><br>        <span class="hljs-punctuation">]</span><br>        <span class="hljs-punctuation">&#125;</span><br>    <span class="hljs-punctuation">]</span><br><span class="hljs-punctuation">&#125;</span><br></code></pre></td></tr></table></figure><p>Next, go to <code>IAM &gt; Users</code> to create a new user and attach the above policy. Once the user is created, issue Access Keys under <code>Command Line Interface</code> access.</p><p>Back to local terminal, use the following command to set up the profile:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">&gt; </span><span class="language-bash">aws configure</span><br>AWS Access Key ID [********************]:<br>AWS Secret Access Key [********************]:<br>Default region name [*************]: <br>Default output format [****]: <br></code></pre></td></tr></table></figure><p>For security, instead of hardcoding access keys, load them dynamically. Add this to <code>.bashrc</code> or <code>.zshrc</code>:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">export</span> AWS_ACCESS_KEY_ID=$(aws configure get aws_access_key_id)<br><span class="hljs-built_in">export</span> AWS_SECRET_ACCESS_KEY=$(aws configure get aws_secret_access_key)<br></code></pre></td></tr></table></figure><p>Apply the changes:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">source</span> ~/.zshrc<br></code></pre></td></tr></table></figure><p>S3 setup is now complete.</p><h2 id="Applying-duckdbrc"><a href="#Applying-duckdbrc" class="headerlink" title="Applying .duckdbrc"></a>Applying <code>.duckdbrc</code></h2><p>It’s tedious to reattach DBs and input env variables every time the UI restarts.<br>You can automate this by adding a <code>.duckdbrc</code> file in your home directory:</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs sql">ATTACH DATABASE &#123;<span class="hljs-operator">/</span>absolute<span class="hljs-operator">/</span>path<span class="hljs-operator">/</span><span class="hljs-keyword">to</span><span class="hljs-operator">/</span>.duckdb&#125; <span class="hljs-keyword">AS</span> dw;<br>INSTALL httpfs;<br>LOAD httpfs;<br><span class="hljs-keyword">SET</span> s3_region<span class="hljs-operator">=</span><span class="hljs-string">&#x27;ap-northeast-2&#x27;</span>;<br><span class="hljs-keyword">SET</span> s3_access_key_id<span class="hljs-operator">=</span><span class="hljs-string">&#x27;$&#123;AWS_ACCESS_KEY_ID&#125;&#x27;</span>;<br><span class="hljs-keyword">SET</span> s3_secret_access_key<span class="hljs-operator">=</span><span class="hljs-string">&#x27;$&#123;AWS_SECRET_ACCESS_KEY&#125;&#x27;</span>;<br></code></pre></td></tr></table></figure><p>This file automatically attaches the DB and sets S3 configs.<br>You’ll see the DB consistently attached every time you restart the UI:</p><p><img src="/en/images/10.png" alt="auto attached DB"></p><h2 id="Testing-S3-Table-in-DuckDB-UI"><a href="#Testing-S3-Table-in-DuckDB-UI" class="headerlink" title="Testing S3 Table in DuckDB UI"></a>Testing S3 Table in DuckDB UI</h2><p>Let’s prepare a sample dataset:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># Create sample.csv</span><br><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;id,name,country</span><br><span class="hljs-string">1,Alice,KR</span><br><span class="hljs-string">2,Bob,US</span><br><span class="hljs-string">3,Charlie,JP&quot;</span> &gt; sample.csv<br><br><span class="hljs-comment"># Upload to S3</span><br>aws s3 <span class="hljs-built_in">cp</span> sample.csv s3://your-bucket-name/sample.csv<br></code></pre></td></tr></table></figure><p>Try querying it:</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs text">HTTP Error: HTTP GET error on &#x27;https://your_bucket.s3.amazonaws.com/sample.csv&#x27; (HTTP 400)<br></code></pre></td></tr></table></figure><p>Uh-oh… the query fails!</p><p>Turns out, the <code>s3_region</code> setting in <code>.duckdbrc</code> wasn’t applied correctly.<br>After re-running the setting manually in the UI, the query works fine:</p><p><img src="/en/images/11.png" alt="query result"></p><p>I’ve reported the <a href="https://github.com/duckdb/duckdb-ui/issues/87">issue</a> in their GitHub repo.</p><hr><p><strong>[2025-03-29 UPDATE]</strong><br>As discussed in the issue, we confirmed that setting <code>s3_region</code> as a <code>GLOBAL</code> variable works correctly.</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs sql">ATTACH DATABASE &#123;<span class="hljs-operator">/</span>absolute<span class="hljs-operator">/</span>path<span class="hljs-operator">/</span><span class="hljs-keyword">to</span><span class="hljs-operator">/</span>.duckdb&#125; <span class="hljs-keyword">AS</span> dw;<br>INSTALL httpfs;<br>LOAD httpfs;<br><span class="hljs-keyword">SET</span> <span class="hljs-keyword">GLOBAL</span> s3_region<span class="hljs-operator">=</span><span class="hljs-string">&#x27;ap-northeast-2&#x27;</span>;<br><span class="hljs-keyword">SET</span> s3_access_key_id<span class="hljs-operator">=</span><span class="hljs-string">&#x27;$&#123;AWS_ACCESS_KEY_ID&#125;&#x27;</span>;<br><span class="hljs-keyword">SET</span> s3_secret_access_key<span class="hljs-operator">=</span><span class="hljs-string">&#x27;$&#123;AWS_SECRET_ACCESS_KEY&#125;&#x27;</span>;<br></code></pre></td></tr></table></figure><p>Add the <code>GLOBAL</code> keyword before <code>s3_region</code> in your <code>.duckdbrc</code> file.</p><hr><p>Let’s continue testing.</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">CREATE TABLE</span> users <span class="hljs-keyword">AS</span><br><span class="hljs-keyword">SELECT</span> <span class="hljs-operator">*</span> <span class="hljs-keyword">FROM</span> read_csv_auto(<span class="hljs-string">&#x27;s3://your-bucket-name/sample.csv&#x27;</span>);<br></code></pre></td></tr></table></figure><p>Now, let’s create a <code>users</code> view based on the <code>CSV</code> file:</p><p><img src="/en/images/12.png" alt="create table"></p><p>Insert and Update the data:</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">UPDATE</span> users<br><span class="hljs-keyword">SET</span> name <span class="hljs-operator">=</span> <span class="hljs-string">&#x27;Bob Updated&#x27;</span>, country <span class="hljs-operator">=</span> <span class="hljs-string">&#x27;CA&#x27;</span><br><span class="hljs-keyword">WHERE</span> id <span class="hljs-operator">=</span> <span class="hljs-number">2</span>;<br><br><span class="hljs-keyword">INSERT INTO</span> users (id, name, country) <span class="hljs-keyword">VALUES</span> (<span class="hljs-number">4</span>, <span class="hljs-string">&#x27;David&#x27;</span>, <span class="hljs-string">&#x27;CN&#x27;</span>);<br></code></pre></td></tr></table></figure><p>These changes affect only the <code>local</code> users table, not the S3 file.<br>To persist the changes back to S3:</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">COPY</span> users <span class="hljs-keyword">TO</span> <span class="hljs-string">&#x27;s3://your-bucket-name/updated_users.parquet&#x27;</span> (FORMAT PARQUET);<br></code></pre></td></tr></table></figure><p><img src="/en/images/13.png" alt="create new table"></p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>Building your own data warehouse with DuckDB and S3 is surprisingly simple yet powerful.<br>The combination of DuckDB’s lightness and S3’s scalability allows you to set up a large-scale data analysis environment locally, without the need for dedicated cloud data warehouses.</p><p>To be clear, this means there are no storage limitations—but it doesn’t mean it can replace a distributed processing engine.<br>Given the nature of a single-node setup, available memory is inevitably limited.<br>However, with proper table design—such as partitioning large tables and only loading the necessary time range for analysis—<br>it’s definitely possible to build a practical and efficient environment.</p>]]></content>
    
    
    <categories>
      
      <category>BigData</category>
      
      <category>engineering</category>
      
    </categories>
    
    
    <tags>
      
      <tag>duckdb</tag>
      
      <tag>datawarehouse</tag>
      
      <tag>data</tag>
      
      <tag>s3</tag>
      
      <tag>aws</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Do You Know About the Spark TimestampNTZ Data Type?</title>
    <link href="/en/en/Engineering/engineering-2/"/>
    <url>/en/en/Engineering/engineering-2/</url>
    
    <content type="html"><![CDATA[<p>Sharing an issue that occurred due to the introduction of the <code>TimestampNTZ</code> data type in environments using Spark and Delta.</p><span id="more"></span><h2 id="EMR-Version-Upgrade"><a href="#EMR-Version-Upgrade" class="headerlink" title="EMR Version Upgrade"></a>EMR Version Upgrade</h2><p>Recently, I upgraded the EMR version in use from 6.10 to 6.15. The reason for the urgent upgrade was an issue in Delta 2.2.0, included in EMR 6.10, where <code>alter</code> statements did not apply new changes. (For example, column names were not updated, or all values were returned as NULL.) If you are using Delta and experiencing the same issue, it is likely a bug in that specific version, so upgrading to a newer version is recommended.</p><p>With the EMR upgrade, the Spark and Delta versions were updated as follows:</p><ul><li>Spark: 3.3.0 → 3.4.1</li><li>Delta: 2.2.0 → 2.4.0</li></ul><p>Before proceeding, I checked the release notes for each version and concluded that there should be no issues since we were not using <code>TimestampNTZ</code>.</p><blockquote><p>Support the TimestampNTZ data type added in Spark 3.3. Using TimestampNTZ requires a Delta protocol upgrade; see the documentation for more information.</p></blockquote><p>The upgrade was completed, and as expected, there were no issues when testing with pyspark or SQL.</p><h2 id="Unexpected-Timestamp-Issue"><a href="#Unexpected-Timestamp-Issue" class="headerlink" title="Unexpected Timestamp Issue"></a>Unexpected Timestamp Issue</h2><p>The next day, our AI engineer reported an issue where <code>timestamp</code> values appeared incorrectly. After running a batch process with the same code as the previous day, he found that the timestamps were displayed in UTC. I quickly checked other batch jobs, but no similar issues were found.</p><p>To get straight to the point, the issue occurred when <code>timestamp</code> data was created in <code>pandas</code> and then inserted into a <code>delta</code> table.<br>Like many other platforms, our system stores data in UTC and converts it to KST only when reading. However, in this process, the data was converted to <code>UTC</code>, stored as <code>TimestampNTZ</code>, and then read as is in <code>Spark</code>.</p><p>That’s right—since <code>delta</code> started supporting <code>TimestampNTZ</code>, if a dataset has no explicit timezone information, the schema is inferred as this data type.</p><h2 id="What-is-the-TimestampNTZ-Data-Type"><a href="#What-is-the-TimestampNTZ-Data-Type" class="headerlink" title="What is the TimestampNTZ Data Type?"></a>What is the TimestampNTZ Data Type?</h2><ul><li>A new timezone-free data type supported in Spark 3.3+</li><li>Unlike the traditional timestamp type, it does not include timezone information and returns values exactly as stored</li><li>Officially supported in Delta 2.4.0+</li></ul><table><thead><tr><th>Data Type</th><th>Timezone Included</th><th>How Spark Converts</th></tr></thead><tbody><tr><td>Timestamp</td><td>Includes timezone</td><td>Converts to the configured timezone when queried</td></tr><tr><td>TimestampNTZ</td><td>Does not include timezone</td><td>Returns stored values as-is</td></tr></tbody></table><p>Therefore, if data is stored as this type, queries must explicitly cast it to a specific timezone.<br>However, if the timezone of the stored <code>TimestampNTZ</code> data is unknown, is casting even meaningful?</p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><ul><li>In Spark 3.3+ and Delta 2.4.0+, TimestampNTZ is enabled by default.</li><li>If pandas datetime data is inserted into Delta without a timezone setting, it is automatically stored as TimestampNTZ.</li><li>When querying TimestampNTZ data in Spark, time conversion does not happen automatically.</li></ul><p>Therefore, the data type should be explicitly defined before storage, or Spark should be configured to handle it explicitly.</p>]]></content>
    
    
    <categories>
      
      <category>BigData</category>
      
      <category>engineering</category>
      
    </categories>
    
    
    <tags>
      
      <tag>BigData</tag>
      
      <tag>Spark</tag>
      
      <tag>Delta</tag>
      
      <tag>TimestampNTZ</tag>
      
      <tag>DataType</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>My Experience Translating 『Julia for Data Analytics』</title>
    <link href="/en/en/Julia/julia-0/"/>
    <url>/en/en/Julia/julia-0/</url>
    
    <content type="html"><![CDATA[<p>In this post, I share my experience of translating an IT book for the first time — how I got started, the challenges I faced, and the lessons I learned along the way.</p><span id="more"></span><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>The book I translated was published in March 2024. Although this reflection comes quite late, I only recently found the time to look back on the experience as I settled down from my hectic routine. I’m not sure if I’ll have another chance to translate a book in the future, but just in case, I wanted to document both the difficulties and rewarding moments of the process.</p><p>Surprisingly, I was offered the opportunity to translate this book when the publisher reached out to me. In 2020, I was studied Julia and had written various blog posts about it. It seems the publisher discovered my posts and decided to contact me. Since it was a publisher whose IT books I had often purchased and read, I found the whole situation quite surreal. There was no reason for me to turn down such a great opportunity. I had a vague idea that it wouldn’t be hard to translate, because I’ve read so many tech articles and books in English. Without much hesitation, I immediately replied to the publisher’s team leader, expressing my interest in the project.</p><p><img src="/en/images/6.jpeg" alt="Plz Think more"></p><p>I should have thought about it more carefully.</p><h2 id="Challenges-During-Translation"><a href="#Challenges-During-Translation" class="headerlink" title="Challenges During Translation"></a>Challenges During Translation</h2><p>At first, I assumed I could approach the translation the same way I studied technical materials.<br>However, this was the naive confidence of someone who knew nothing about translation.</p><p>The first challenge was converting English technical terms into appropriate Korean equivalents. Many terms are commonly used in English within the industry, even in Korean-speaking contexts. For example, should ecosystem and named tuple be translated, or should they remain in English? How should I translate between parameter and argument in Korean? When should I include both English and Korean terms together (i.e., provide a transliteration or explanation)? Even choosing basic terms was more difficult than I thought.</p><p>English text is often structured with long, complex sentences, frequently using <code>which</code> or <code>that</code> to connect ideas. In contrast, Korean generally flows better when broken into two or three separate sentences. Additionally, English adjectives often follow nouns, whereas in Korean, they usually precede them. If I translated English text word-for-word, I ended up with what I call <code>heavy-headed sentences</code> that were difficult to read. However, if I broke up the sentences too much, I feared that I might alter the author’s intended meaning. Early on, I sometimes spent 30 minutes agonizing over a single sentence. But as I progressed, I naturally developed my own strategies and guidelines for handling these issues.</p><p>The third is something every office worker faces: the battle for time. Since my company is based at home, I thought I had a lot of time on my hands becuase i didn’t have to commute to the office. However, as an office worker, you have a busy day just managing your work and home. When work is busy, home is relaxed, and when work is relaxed, there are often incidents at home. I was no different. When I was busy at work, I was rushing to get things done, and when I was done, there were home issues that needed to be addressed, such as a sick family member who needed to be cared for. Translation requires long, uninterrupted sessions to ensure a smooth and consistent tone across chapters. With a looming deadline, I had to push through and complete the translation despite all these challenges.</p><h2 id="What-I-learned-from-translating"><a href="#What-I-learned-from-translating" class="headerlink" title="What I learned from translating"></a>What I learned from translating</h2><p>But if you ask me, “Do you regret working on the translation?”, of course not.<br>On the contrary, I feel honored to have been given this opportunity.</p><p>Translating forced me to revisit <code>basic concepts</code> that I had previously glossed over using industry jargon.<br>By putting these terms into my own words, I was able to internalize them more effectively and understand them much more deeply. I think the saying “if you really want to understand something, explain it in your own words” is so true.</p><p>In the process, my knowledge of <code>Julia</code> has greatly improved. Before this book, I learned Julia mainly by reading documentation and experimenting on my own. But this book systematically explains Julia’s ecosystem and practical usage, introducing me to many concepts I hadn’t encountered before. It also made me realize how much the language has evolved since I first started using it in 2020.</p><p>This experience gave me a glimpse into the world of <code>translation</code>, which I believe is a very rare and valuable opportunity. I gained first-hand knowledge of how book translation projects are initiated, negotiated, and executed, and gained insights that I would not have gotten otherwise.</p><h2 id="Final-Thoughts"><a href="#Final-Thoughts" class="headerlink" title="Final Thoughts"></a>Final Thoughts</h2><p>I am so grateful that I was able to have this experience, even though I have no experience in translation and am in a junior career in some ways. And also I’m grateful to the publisher and other people who worked with me on this project, even though I was a beginner in every aspect. When the next translation opportunity comes, I will study a lot so that I can do a better translation.</p><p>For those of you who are curious about what the translated book is like, here is the <a href="https://blog.naver.com/jeipubmarketer/223381130430">book introduction page</a>.</p>]]></content>
    
    
    <categories>
      
      <category>Language</category>
      
      <category>julia</category>
      
    </categories>
    
    
    <tags>
      
      <tag>julia</tag>
      
      <tag>translate</tag>
      
      <tag>translation</tag>
      
      <tag>dataanalysis</tag>
      
      <tag>JuliaforDataAnalysis</tag>
      
      <tag>book</tag>
      
      <tag>korean</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Please Build a Data Warehouse</title>
    <link href="/en/en/Engineering/engineering-1/"/>
    <url>/en/en/Engineering/engineering-1/</url>
    
    <content type="html"><![CDATA[<p>As a data engineer, I’d like to share the importance of a proper data warehouse and how to construct one effectively, based on my experience.</p><span id="more"></span><h2 id="Introduction-Pouring-Water-into-a-Bottomless-Pit"><a href="#Introduction-Pouring-Water-into-a-Bottomless-Pit" class="headerlink" title="Introduction: Pouring Water into a Bottomless Pit"></a>Introduction: Pouring Water into a Bottomless Pit</h2><p>It’s already been four years since I started my career as a data engineer, and I’ve been working at my current company for almost three years now. When I first joined, the system relied heavily on duplicating RDS for creating data products. But over time, we’ve stabilized our data lake using the Medallion architecture, allowing us to focus on improving the quality of data products.</p><p>In this post, I’ll summarize the efforts made throughout 2024 to build a data warehouse and improve data quality.</p><p>From conversations with others, I’ve often heard the following reasons why data warehouses are perceived as a double-edged sword:</p><ul><li>Most data teams are small and too busy to handle such projects.</li><li>Setting up and properly using a data warehouse requires significant resources.</li><li>Adding another layer before the product feels unnecessarily cumbersome.</li><li>Some people still don’t fully understand why it’s needed given the resources required.</li></ul><p>So some companies didn’t have one at all, and most of them just organized it as One Big Table (OBT) or preprocessed the data types of frequently used tables and called it a data warehouse.</p><p>Our company was no exception. Without a proper data warehouse, dashboards and metrics were calculated differently by per people, leading to inconsistent results across products. Meanwhile, technical debt kept piling up.</p><p>In such an environment, constantly maintaining systems felt like <code>pouring water into a bottomless pit</code>.</p><p><img src="/en/images/5.png" alt="WHAT I DO"></p><h2 id="Why-Do-We-Need-a-Data-Warehouse"><a href="#Why-Do-We-Need-a-Data-Warehouse" class="headerlink" title="Why Do We Need a Data Warehouse?"></a>Why Do We Need a Data Warehouse?</h2><p>The absence of a data warehouse leads to the following major issues:</p><ol><li><p>Data quality management becomes challenging.</p><ul><li>Developers apply different metric definitions, causing confusion.</li><li>Changes in backend structures are often not communicated to the data team, leading to outdated data being used.</li><li>When metric logic changes, it’s hard to identify all the areas that need updates.</li></ul></li><li><p>Managing pipeline code becomes difficult.</p><ul><li>With everyone creating their own pipelines, maintenance becomes impossible if someone leaves the company.</li><li>Even within the data team, it’s unclear what products exist.</li><li>Maintenance consumes excessive time and resources.</li></ul></li></ol><p>A data warehouse plays a central role in managing data quality and maintaining efficient code. For example, when defining GMV for our company, some people calculated it using <code>pay_time</code>, while others used <code>create_time</code>. Such inconsistencies erode trust and reliability in the data. By defining a standard GMV formula and adding a GMV column to a warehouse table, we ensured all products pulled the value from the same source. Later, if the GMV calculation formula changes, updating the warehouse pipeline alone ensures consistency across all metrics.</p><p>Additionally, when backend logic changes or server issues cause data integrity problems, the warehouse manager can quickly address them. Without this centralization, different data products with independent pipelines create an exponential increase in maintenance tasks, rendering the system unmanageable. Over time, this results in significant legacy systems—akin to museum relics—that exist solely for troubleshooting. A data warehouse prevents such issues and provides an excellent solution.</p><p>Our company decided to address these challenges by building a proper data warehouse.</p><h2 id="How-to-Build-a-Data-Warehouse"><a href="#How-to-Build-a-Data-Warehouse" class="headerlink" title="How to Build a Data Warehouse"></a>How to Build a Data Warehouse</h2><p>So, how do you get started with building a data warehouse? The first step is to <code>understand the service domains</code>. In our company, multiple domains exist within a single large service, each with distinct business logic. Understanding these business rules is essential to define the scope during data modeling.</p><p>The next step is to <code>understand the operational data</code> tied to the business logic. Analytical data is fundamentally derived from service-generated data. Without understanding the service database logic, building a warehouse is impossible. This involves consulting with the backend team, reading historical documentation, and directly querying and exploring the schemas.</p><p>Once you’ve studied the data, you can move to the development phase.</p><p>The third step is to <code>design your data modeling</code>. This involves choosing between a star schema and a snowflake schema based on the business logic and data. For more details on these schema structures, check out <a href="https://dev-bearabbit.github.io/en/en/Engineering/engineering-0/">this post</a>.</p><p>Some tips to keep in mind during modeling:</p><ul><li>Define common metrics first and modeling around them.</li><li>Use logical primary keys for table integration.</li><li>Add data integrity labels (e.g., VALID, BAD_DATA, INVALID).</li><li>Document your data model thoroughly.</li><li>Include separate columns for INSERT and UPDATE timestamps.</li></ul><p>Once you’re done modeling your data, the next step is to <code>design your ETL/ELT pipeline</code>. The pipeline should reflect the modeling results, supporting either INSERT or UPDATE operations. Choosing a platform that supports UPSERT is ideal. Our team uses Spark with the open-source Delta Lake library to implement UPSERT operations. Additionally, we set up Airflow to run daily batches, ensuring that the latest data is regularly updated or added. Creating pipeline templates can save time when configuring DAGs repeatedly.</p><p>Next, you need to configure an environment to <code>organize and share the table schemas in your warehouse</code>. Organizing and sharing table schemas within the warehouse is essential. Usually, a data catalog platform is used for this purpose. Our team uses DataHub to manage and share schemas. Without documenting tables during modeling, you’ll face a mountain of overdue documentation tasks later.</p><p>The final step is to <code>organize your quality control and testing system</code>. The warehouse assumes its data is high-quality by default, so maintaining quality control is crucial. Engineers responsible for the warehouse should continually think about quality and testing. This includes:</p><ul><li>Data integrity tests</li><li>NULL value checks</li><li>Duplicate data removal</li><li>Monitoring historical changes and outliers</li></ul><p>In reality, business logic and operational data logic constantly evolve as services grow and change. Ultimately, data quality management involves recognizing, understanding, and consistently reflecting these changes. For this reason, building and maintaining a data warehouse requires significant resources. But if you build a product without these filters in place, you’re more likely to become a mass-production shop that accumulates technical debt that’s hard to reverse at scale.</p><p>To summarize, the process can be broken down as follows:</p><ol><li><p>Learning Phase</p><ul><li>Understand the business logic for each service domain.</li><li>Understand the operational data and database logic for the service.</li></ul></li><li><p>Development Phase</p><ul><li>Design the data model.</li><li>Design and develop the ETL&#x2F;ELT pipelines.</li><li>Build a data catalog.</li><li>Establish quality control and testing systems.</li></ul></li></ol><h2 id="Conclusion-Please-Build-a-Data-Warehouse"><a href="#Conclusion-Please-Build-a-Data-Warehouse" class="headerlink" title="Conclusion: Please Build a Data Warehouse"></a>Conclusion: Please Build a Data Warehouse</h2><p>A data warehouse forms the backbone of a data team, ensuring data quality and simplifying code maintenance. Addressing the aforementioned issues and goals can significantly enhance a data team’s productivity and efficiency.</p><p>A data warehouse is not just a storage solution but the heart of the data team, systematically managing data and providing reliable information. If infrastructure like this isn’t built when the system is manageable, the team will likely descend into maintenance hell.</p><p>So, the conclusion of this post is simple: please build a data warehouse early to prevent chaos down the road.</p>]]></content>
    
    
    <categories>
      
      <category>BigData</category>
      
      <category>engineering</category>
      
    </categories>
    
    
    <tags>
      
      <tag>BigData</tag>
      
      <tag>DataWarehouse</tag>
      
      <tag>DW</tag>
      
      <tag>DataQuality</tag>
      
      <tag>Pipeline</tag>
      
      <tag>DataEngineering</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Avoid a Cost Explosion with AWS Personalize</title>
    <link href="/en/en/Lesson/lesson-0/"/>
    <url>/en/en/Lesson/lesson-0/</url>
    
    <content type="html"><![CDATA[<p>In this post, I’ll share how I unexpectedly faced a cost explosion while using AWS Personalize. Hopefully, my experience can help others avoid similar pitfalls in SaaS cost management.</p><span id="more"></span><hr><h2 id="The-Incident"><a href="#The-Incident" class="headerlink" title="The Incident"></a>The Incident</h2><p>I was developing and testing a tool for extracting CRM campaign audiences. As part of this process, I compared AWS Personalize pricing, estimated the expected costs, and documented everything in a Confluence file to present to my lead. After approval, we moved forward with A&#x2F;B testing between our current method and the model output. For context, I used the <code>aws-item-affinity</code> recipe for item-user segmentation.</p><p>However, this morning, I received an alert about a sudden surge in Personalize costs. According to my calculations, the expenses should have been minimal. What went wrong? My spine chilled as I felt something was terribly off. Feeling a wave of panic, I immediately began investigating.</p><p><img src="/en/images/0.png" alt="What’s going on"></p><p>I spent the entire morning staring at the <a href="https://aws.amazon.com/personalize/pricing/">AWS Personalize pricing page</a> and recalculating costs based on usage. Finally, I identified the issue.</p><h2 id="Root-Cause"><a href="#Root-Cause" class="headerlink" title="Root Cause"></a>Root Cause</h2><p>The culprit was the batch segment job. While the dataset and training costs were minimal since they involved one-time charges, the batch segment cost was the bombshell. Let’s take a look at the pricing details from AWS’s official page.</p><p><img src="/en/images/1.png" alt="Pricing table for batch segment"></p><p>The table above outlines how batch segment costs are calculated. Here’s where I misunderstood: I assumed the term Users in dataset referred to the number of users requested per segment. That is, for up to 100,000 requests, the cost would be <code>$0.016</code> per segment request, and for 100,001–900,000 requests, the cost would drop to <code>$0.008</code>. Based on this, I calculated the cost for retrieving 10,000 users for each of 10 items like this:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-number">10000</span>/<span class="hljs-number">1000</span> * <span class="hljs-number">0.016</span> * <span class="hljs-number">10</span><br></code></pre></td></tr></table></figure><p>However, <code>Users in dataset</code> actually refers to <code>Total Users in the Dataset</code>—the total number of users in the training dataset—irrespective of the number of users requested in the segment job. Since I trained the model on a year’s worth of data, the dataset contained approximately 600,000 users. The correct calculation, then, is:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">Tier_1 = <span class="hljs-number">100000</span>/<span class="hljs-number">1000</span> * <span class="hljs-number">0.016</span> * <span class="hljs-number">10</span><br>Tier_2 = <span class="hljs-number">500000</span>/<span class="hljs-number">1000</span> * <span class="hljs-number">0.008</span> * <span class="hljs-number">10</span><br></code></pre></td></tr></table></figure><p>In my initial understanding, the cost would have been just <code>$1.6</code>, but the actual cost came to <code>$56</code>. This was entirely my mistake for not double-checking the details. Looking back, I should have questioned why the costs seemed so low and revisited the documentation for clarity. At the same time, I can’t help but feel like AWS’s pricing table is designed to be confusing—it never crossed my mind that the requested number of users wouldn’t factor into the calculation at all.</p><p><img src="/en/images/2.png" alt="Plz undo everything"></p><p>If you miscalculate the cost of using SaaS in the cloud ecosystem, you’ll end up paying a lot of unnecessary money, even if your app works. I wrote my first postmortem documentation as a developer because of this cost. I felt sad because I made a mistake I didn’t expect. For anyone confused like me when calculating costs, I’m leaving this on my blog.</p><h2 id="Lessons-Learned"><a href="#Lessons-Learned" class="headerlink" title="Lessons Learned"></a>Lessons Learned</h2><ul><li>Cloud costs are seriously scary. Sometimes, I wonder if server crashes would be stressful… Actually, no—that’s a nightmare too.</li><li>Always read AWS documentation carefully and double-check with others to avoid misunderstandings.</li><li>For AWS Personalize, pay close attention to segment costs. The size of the training dataset directly affects batch segment charges.</li></ul>]]></content>
    
    
    <categories>
      
      <category>etc.</category>
      
      <category>lessons</category>
      
    </categories>
    
    
    <tags>
      
      <tag>aws</tag>
      
      <tag>personalize</tag>
      
      <tag>pricing</tag>
      
      <tag>mistake</tag>
      
      <tag>aws-item-affinity</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Customizing Hexo for a Multilingual Blog</title>
    <link href="/en/en/AboutHexo/about-hexo-0/"/>
    <url>/en/en/AboutHexo/about-hexo-0/</url>
    
    <content type="html"><![CDATA[<p>Here’s how to customize your Hexo blog to support multiple languages.</p><span id="more"></span><hr><h2 id="Current-Status"><a href="#Current-Status" class="headerlink" title="Current Status"></a>Current Status</h2><p>Here’s the current state of my technical blog:</p><ul><li>Hexo with the Fluid theme is being used.</li><li>The Fluid theme does not natively support multiple languages.</li><li>Custom development is needed to enable multilingual functionality.</li></ul><h2 id="Customization-Requirements"><a href="#Customization-Requirements" class="headerlink" title="Customization Requirements"></a>Customization Requirements</h2><p>The customization requirements were as follows:</p><ul><li>The root URL should serve the Korean version, and the English version should be redirected to the &#x2F;en&#x2F; path.</li><li>Posts should be written independently for each language, with categories and tags tailored to the specific language.</li><li>A language selection dropdown should be added to the navbar, allowing users to switch languages.</li></ul><h2 id="Customization-Setup"><a href="#Customization-Setup" class="headerlink" title="Customization Setup"></a>Customization Setup</h2><p>The customization process was straightforward: create <strong>two separate Hexo blogs</strong> using the same theme and modify only the navbar.<br>To meet the requirements, the blog deployment and source code folders were structured as follows:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">drwxr-xr-x  18 ryu  staff   576  1  2 06:33 deploy<br>drwxr-xr-x  13 ryu  staff   416  1  2 05:22 en-blog<br>drwxr-xr-x  11 ryu  staff   352  1  3 01:47 ko-blog<br></code></pre></td></tr></table></figure><ul><li><code>en-blog</code>: Source code for the English blog</li><li><code>ko-blog</code>: Source code for the Korean blog</li><li><code>deploy</code>: Final folder containing files ready for deployment</li></ul><p>If you’re starting from scratch, create the blog source folders using the following commands:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">hexo init ko-blog<br>hexo init en-blog<br></code></pre></td></tr></table></figure><p>Then navigate to each folder, download the desired theme, and set the default configuration separately.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># Korean version</span><br><span class="hljs-built_in">cd</span> ko-blog<br>npm install --save hexo-theme-fluid<br><br><span class="hljs-comment"># English version</span><br><span class="hljs-built_in">cd</span> en-blog<br>npm install --save hexo-theme-fluid<br></code></pre></td></tr></table></figure><p>Since I planned to use the Korean version as the root URL, I configured the <code>_config.yml</code> files as follows:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># Korean version: /ko-blog/_config.yml</span><br>language: ko<br>url: https://dev-bearabbit.github.io<br>root: /<br>permalink: :lang/:title/<br><br><span class="hljs-comment"># English version: /en-blog/_config.yml</span><br>language: en<br>root: /en/<br>permalink: :lang/:title/<br></code></pre></td></tr></table></figure><p>This configuration adds the lang parameter to the permalink, so it’s essential to include lang in each post. To simplify this, I modified the default post scaffolds for each blog:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># Korean version: /ko-blog/scaffolds/post.md</span><br>---<br>title: &#123;&#123; title &#125;&#125;<br>lang: ko<br><span class="hljs-built_in">date</span>: &#123;&#123; <span class="hljs-built_in">date</span> &#125;&#125;<br>tags:<br>categories:<br>---<br><br><span class="hljs-comment"># English version: /en-blog/scaffolds/post.md</span><br>---<br>title: &#123;&#123; title &#125;&#125;<br>lang: en<br><span class="hljs-built_in">date</span>: &#123;&#123; <span class="hljs-built_in">date</span> &#125;&#125;<br>tags:<br>categories:<br>---<br></code></pre></td></tr></table></figure><p>Next, I added a language selection dropdown to the navbar by editing the theme’s navigation files. In the Fluid theme, this can be done in <code>layout/_partials/header/navigation.ejs</code>.</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs html"><span class="hljs-comment">&lt;!-- Korean version: /ko-blog/themes/fluid/layout/_partials/header/navigation.ejs --&gt;</span><br><span class="hljs-comment">&lt;!-- English version: /en-blog/themes/fluid/layout/_partials/header/navigation.ejs --&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">div</span> <span class="hljs-attr">class</span>=<span class="hljs-string">&quot;dropdown-menu&quot;</span> <span class="hljs-attr">aria-labelledby</span>=<span class="hljs-string">&quot;languageDropdown&quot;</span>&gt;</span><br>  <span class="hljs-tag">&lt;<span class="hljs-name">a</span> <span class="hljs-attr">class</span>=<span class="hljs-string">&quot;dropdown-item&quot;</span> <span class="hljs-attr">href</span>=<span class="hljs-string">&quot;/&quot;</span>&gt;</span>한국어<span class="hljs-tag">&lt;/<span class="hljs-name">a</span>&gt;</span><br>  <span class="hljs-tag">&lt;<span class="hljs-name">a</span> <span class="hljs-attr">class</span>=<span class="hljs-string">&quot;dropdown-item&quot;</span> <span class="hljs-attr">href</span>=<span class="hljs-string">&quot;/en/&quot;</span>&gt;</span>English<span class="hljs-tag">&lt;/<span class="hljs-name">a</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">div</span>&gt;</span><br></code></pre></td></tr></table></figure><p>To display the selected language in the navbar, I added the following script:</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs html"><span class="hljs-tag">&lt;<span class="hljs-name">script</span>&gt;</span><span class="language-javascript"></span><br><span class="language-javascript">  <span class="hljs-variable language_">document</span>.<span class="hljs-title function_">addEventListener</span>(<span class="hljs-string">&#x27;DOMContentLoaded&#x27;</span>, <span class="hljs-function">() =&gt;</span> &#123;</span><br><span class="language-javascript">    <span class="hljs-keyword">const</span> path = <span class="hljs-variable language_">window</span>.<span class="hljs-property">location</span>.<span class="hljs-property">pathname</span>;</span><br><span class="language-javascript">    <span class="hljs-keyword">const</span> currentLang = path.<span class="hljs-title function_">startsWith</span>(<span class="hljs-string">&#x27;/en/&#x27;</span>) ? <span class="hljs-string">&#x27;English&#x27;</span> : <span class="hljs-string">&#x27;한국어&#x27;</span>;</span><br><span class="language-javascript">    <span class="hljs-keyword">const</span> currentLangElement = <span class="hljs-variable language_">document</span>.<span class="hljs-title function_">getElementById</span>(<span class="hljs-string">&#x27;current-lang&#x27;</span>);</span><br><span class="language-javascript">    currentLangElement.<span class="hljs-property">textContent</span> = currentLang;</span><br><span class="language-javascript">  &#125;);</span><br><span class="language-javascript"></span><span class="hljs-tag">&lt;/<span class="hljs-name">script</span>&gt;</span><br></code></pre></td></tr></table></figure><p>Finally, I updated the <code>head.ejs</code> file to include alternate language links:</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs html"><span class="hljs-comment">&lt;!-- Korean version: /ko-blog/themes/fluid/layout/_partials/head.ejs --&gt;</span><br><span class="hljs-comment">&lt;!-- English version: /en-blog/themes/fluid/layout/_partials/head.ejs --&gt;</span><br><br><span class="hljs-tag">&lt;<span class="hljs-name">link</span> <span class="hljs-attr">rel</span>=<span class="hljs-string">&quot;alternate&quot;</span> <span class="hljs-attr">hreflang</span>=<span class="hljs-string">&quot;ko&quot;</span> <span class="hljs-attr">href</span>=<span class="hljs-string">&quot;https://dev-bearabbit.github.io/&quot;</span> /&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">link</span> <span class="hljs-attr">rel</span>=<span class="hljs-string">&quot;alternate&quot;</span> <span class="hljs-attr">hreflang</span>=<span class="hljs-string">&quot;en&quot;</span> <span class="hljs-attr">href</span>=<span class="hljs-string">&quot;https://dev-bearabbit.github.io/en/&quot;</span> /&gt;</span><br></code></pre></td></tr></table></figure><p>With these changes, the customization is complete.</p><h2 id="Deploying-the-Blog"><a href="#Deploying-the-Blog" class="headerlink" title="Deploying the Blog"></a>Deploying the Blog</h2><p>The blog is deployed using GitHub Pages. First, generate the deployment files for both blogs:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># Korean version: ko-blog</span><br><span class="hljs-comment"># English version: en-blog</span><br>hexo clean<br>hexo generate<br></code></pre></td></tr></table></figure><p>The generated files will be located in the following directories:</p><ul><li>Korean version: ko-blog&#x2F;public&#x2F;</li><li>English version: en-blog&#x2F;public&#x2F;</li></ul><p>Move the files into the <code>deploy</code> folder. Ensure the Korean version is moved to the root of <code>deploy</code>, while the English version is moved to <code>deploy/en/</code>.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">rm</span> -rf deploy/*<br><br><span class="hljs-built_in">mv</span> ko-blog/public/* deploy/<br><span class="hljs-built_in">mkdir</span> -p deploy/en/ &amp;&amp; <span class="hljs-built_in">mv</span> en-blog/public/* deploy/en/<br></code></pre></td></tr></table></figure><p>Finally, deploy the contents of the deploy folder to GitHub.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># Initial setup</span><br><span class="hljs-built_in">cd</span> deploy<br>git init<br>git remote add origin https://github.com/&lt;username&gt;/&lt;repository&gt;.git<br><br><span class="hljs-comment"># Push changes</span><br>git add .<br>git commit -m <span class="hljs-string">&quot;Deploy folder initial commit&quot;</span><br>git push origin main<br></code></pre></td></tr></table></figure><p>This completes the setup for a multilingual Hexo blog.</p>]]></content>
    
    
    <categories>
      
      <category>etc.</category>
      
      <category>about hexo</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Hexo</tag>
      
      <tag>blog</tag>
      
      <tag>multilingual</tag>
      
      <tag>custom</tag>
      
      <tag>English blog</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Basic Structure of Analytical Tables</title>
    <link href="/en/en/Engineering/engineering-0/"/>
    <url>/en/en/Engineering/engineering-0/</url>
    
    <content type="html"><![CDATA[<p>Before I begin, I’d like to note that this article is summarized and organized from Keisuke Nishida’s book Big Data Technology.</p><span id="more"></span><h2 id="Normalization-and-Denormalization"><a href="#Normalization-and-Denormalization" class="headerlink" title="Normalization and Denormalization"></a>Normalization and Denormalization</h2><p>Typically, relational databases(RDBs) are structured using normalized data, which is generally divided into transaction data and master data. <code>Transaction data</code> refers to data that is recorded sequentially over time, while <code>Master data</code> contains various types of reference information used by transaction data. Transaction data is immutable once recorded, whereas master data may change depending on circumstances.</p><p>The process of organizing data to minimize redundancy is called <code>Normalization</code>. By normalizing data, integrity can be maintained, and the storage size can be optimized.</p><p>However, tables used for analysis often undergo the opposite process called <code>Denormalization</code>. Denormalization involves combining transaction data and master data to make analysis more convenient. To create dimension tables for analysis, normalized tables are recombined into a single table. In this process, data redundancy is acceptable.</p><h2 id="Multidimensional-Model"><a href="#Multidimensional-Model" class="headerlink" title="Multidimensional Model"></a>Multidimensional Model</h2><p>A multidimensional model, as the name suggests, represents data across multiple dimensions. Normally, when we think of data, we envision a two-dimensional model consisting of rows and columns. However, operational data often has much more complex structures, making it challenging to represent it simply in two dimensions. Attempting to do so could result in significant data duplication and an unmanageable number of columns.</p><p>In industry terms, a multidimensional model refers to a design where data is divided into <code>fact tables</code>, which represent actual data, and <code>dimension tables</code>, which provide reference values. This structure minimizes redundancy and helps users better understand the scope of the data.</p><p>Multidimensional models are typically designed based on the metrics to be analyzed. On the other hand, MPP(Massively Parallel Processing) databases do not incorporate the concept of multidimensional models, instead relying on denormalized tables.</p><h2 id="Fact-Table-and-Dimension-Table"><a href="#Fact-Table-and-Dimension-Table" class="headerlink" title="Fact Table and Dimension Table"></a>Fact Table and Dimension Table</h2><p><code>Fact tables</code> record transactional data or observations, while <code>Dimension tables</code> store the reference data used by the fact tables. For example, numerical data used for aggregation is stored in fact tables, and dimension tables are used to classify this data by providing attributes.</p><p>As data volume grows, fact tables become significantly larger than dimension tables, and the size of fact tables directly affects aggregation performance. Therefore, keeping fact tables as small as possible is crucial for high performance. Only keys like IDs are retained in fact tables, with other attributes moved to dimension tables.</p><h2 id="Star-Schema-and-Snowflake-Schema"><a href="#Star-Schema-and-Snowflake-Schema" class="headerlink" title="Star Schema and Snowflake Schema"></a>Star Schema and Snowflake Schema</h2><p>The <code>Star Schema</code> organizes data with a fact table at the center, surrounded by multiple dimension tables. Dimension tables are single-layered and do not have hierarchical structures. The following image illustrates the structure of a star schema:</p><p><img src="/en/images/3.png" alt="star schema"></p><p>The <code>Snowflake Schema</code> extends the star schema by allowing dimension tables to have hierarchical sub-dimensions. In other words, it applies stronger normalization compared to the star schema. The following image shows the structure of a snowflake schema:</p><p><img src="/en/images/4.png" alt="snowflake schmea"></p><p>The comparison between the star schema and the snowflake schema is as follows:</p><table><thead><tr><th>Aspect</th><th>Star Schema</th><th>Snowflake Schema</th></tr></thead><tbody><tr><td>Advantages</td><td>- Simple data structure and queries. <br></td><td>- Minimizes data redundancy and storage space. <br> - Easier to maintain due to the absence of redundancy.</td></tr><tr><td>Disadvantages</td><td>- Dimension tables may have redundancy. <br> - Requires more storage space.</td><td>- Requires more joins, leading to potential performance issues. <br> - More complex data structure.</td></tr></tbody></table><p>Generally, the snowflake schema is more suitable for data warehouses, while the star schema is considered better for data marts.</p><h2 id="Columnar-Storage-and-Denormalized-Tables"><a href="#Columnar-Storage-and-Denormalized-Tables" class="headerlink" title="Columnar Storage and Denormalized Tables"></a>Columnar Storage and Denormalized Tables</h2><p>With the advent of columnar storage systems, the situation has changed. Even if the number of columns increases, the performance of query engines is not significantly affected, and column-level compression reduces disk I&#x2F;O overhead. Consequently, there is less need to separate data into dimension tables, and a single, large fact table may suffice.</p><p>A <code>Denormalized table</code> is created by further denormalizing a star schema and combining all the tables into one fact table. For data warehouses, the star schema remains optimal for table structures. During the data accumulation phase, separating fact and dimension tables is preferable, but when creating data marts, combining them into a denormalized table is often recommended.</p>]]></content>
    
    
    <categories>
      
      <category>BigData</category>
      
      <category>engineering</category>
      
    </categories>
    
    
    <tags>
      
      <tag>BigData</tag>
      
      <tag>DataWarehouse</tag>
      
      <tag>DataMart</tag>
      
      <tag>Schema</tag>
      
      <tag>StarSchema</tag>
      
      <tag>SnowflakeSchema</tag>
      
      <tag>DenormalizedTable</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
