<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>Build Your Own Data Warehouse with DuckDB + S3</title>
    <link href="/en/en/Engineering/engineering-3/"/>
    <url>/en/en/Engineering/engineering-3/</url>
    
    <content type="html"><![CDATA[<p>Introducing an easy way to store data in AWS S3 and query it directly from your local machine using DuckDB.</p><span id="more"></span><h2 id="Why-DuckDB-S3"><a href="#Why-DuckDB-S3" class="headerlink" title="Why DuckDB + S3?"></a>Why DuckDB + S3?</h2><ul><li>DuckDB: Lightweight like SQLite, but with a columnar engine for speed and strong SQL support</li><li>S3: Store large datasets without worry and access them from anywhere, whether local or cloud</li><li>Simple Setup: No complex infrastructure needed—start analyzing data right away</li></ul><h2 id="Architecture-Task-Outline"><a href="#Architecture-Task-Outline" class="headerlink" title="Architecture &amp; Task Outline"></a>Architecture &amp; Task Outline</h2><p>Here’s how the structure looks:</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs text">[S3] Data Storage ──────▶ [DuckDB] Query locally<br>                         └──────▶ Metadata &amp; Schema Management (.duckdb file)<br></code></pre></td></tr></table></figure><p>Actual table data is stored in S3, while metadata and schema files are kept locally.</p><p>This way, you can analyze freely without worrying about storage limits.<br>Also, the schema stays intact while you can update only the data as needed.</p><p>Today’s task order:</p><ul><li>Install DuckDB on Mac</li><li>Learn how to use DuckDB UI</li><li>Connect DuckDB to S3</li><li>Apply .duckdbrc configuration</li><li>Test S3 tables from DuckDB UI</li></ul><h2 id="Installing-DuckDB-on-Mac"><a href="#Installing-DuckDB-on-Mac" class="headerlink" title="Installing DuckDB on Mac"></a>Installing DuckDB on Mac</h2><p>Installation is super simple via terminal:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">&gt; brew install duckdb<br></code></pre></td></tr></table></figure><p>Once installed, you can use it in <code>Transient In-Memory Mode</code> like below:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs bash">&gt; duckdb<br>v1.2.1 8e52ec4395<br>Enter <span class="hljs-string">&quot;.help&quot;</span> <span class="hljs-keyword">for</span> usage hints.<br>Connected to a transient in-memory database.<br>Use <span class="hljs-string">&quot;.open FILENAME&quot;</span> to reopen on a persistent database.<br>D <br></code></pre></td></tr></table></figure><p>DuckDB offers a REPL mode by default. Depending on whether you want one-time use or persistence, choose:</p><ul><li>Transient In-Memory DB: If you connect without <code>.open</code>, data is kept in memory and lost on exit</li><li>Persistent DB: <code>.duckdb</code> file stores schema&#x2F;data and can be reused</li></ul><h2 id="Using-DuckDB-UI"><a href="#Using-DuckDB-UI" class="headerlink" title="Using DuckDB UI"></a>Using DuckDB UI</h2><p>You can launch the UI, which is notebook-style, using a simple CLI command:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs shell">❯ duckdb -ui<br>┌──────────────────────────────────────┐<br>│                result                │<br>│               varchar                │<br>├──────────────────────────────────────┤<br>│ UI started at http://localhost:4213/ │<br>└──────────────────────────────────────┘<br>v1.2.1 8e52ec4395<br>Enter &quot;.help&quot; for usage hints.<br>D <br></code></pre></td></tr></table></figure><p>Visiting <code>http://localhost:4213/</code>, you’ll see a welcome page like this:</p><p><img src="/en/images/7.png" alt="duckdb page"></p><p>Click on <code>attached databases</code> at the top left and create a <code>.duckdb</code> file via the File option.</p><p><img src="/en/images/8.png" alt="add database"></p><p>You’ll see the <code>.duckdb</code> file created at the specified path:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs shell">~/Desktop/duckdb on  main [✘»!+?] on ☁️  (ap-northeast-2) <br>❯ ls -al<br>total 24<br>drwxr-xr-x   3 ryu  staff     96  3 23 20:53 .<br>drwx------@ 24 ryu  staff    768  3 23 20:53 ..<br>-rw-r--r--   1 ryu  staff  12288  3 23 20:53 mywarehouse.duckdb<br></code></pre></td></tr></table></figure><p>DuckDB UI allows three types of database connections:</p><ul><li>File: Connect or create <code>.duckdb</code> files in the local file system</li><li>URL: Useful for sharing read-only DB files in a centralized storage</li><li>Memory: Ideal for tests, one-off queries, or temporary sessions</li></ul><p>The URL option can be really handy in team environments.<br>Engineers can ETL and upload, while analysts&#x2F;managers can use the data.<br>Since it’s read-only, there’s no risk of accidentally deleting data.<br>However, whenever a new table is added or schema changes, someone needs to update the <code>.duckdb</code> file.</p><h2 id="Connecting-DuckDB-to-S3"><a href="#Connecting-DuckDB-to-S3" class="headerlink" title="Connecting DuckDB to S3"></a>Connecting DuckDB to S3</h2><p>First, create an S3 bucket to store your data. Disable all public access options.</p><p><img src="/en/images/9.png" alt="create S3 bucket"></p><p>Then go to <code>IAM &gt; Policies</code> and create a policy like this, allowing the specific bucket access:</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs json"><span class="hljs-punctuation">&#123;</span><br>    <span class="hljs-attr">&quot;Version&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;2012-10-17&quot;</span><span class="hljs-punctuation">,</span><br>    <span class="hljs-attr">&quot;Statement&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">[</span><br>        <span class="hljs-punctuation">&#123;</span><br>        <span class="hljs-attr">&quot;Effect&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;Allow&quot;</span><span class="hljs-punctuation">,</span><br>        <span class="hljs-attr">&quot;Action&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">[</span><br>            <span class="hljs-string">&quot;s3:GetObject&quot;</span><span class="hljs-punctuation">,</span><br>            <span class="hljs-string">&quot;s3:ListBucket&quot;</span><span class="hljs-punctuation">,</span><br>            <span class="hljs-string">&quot;s3:PutObject&quot;</span><span class="hljs-punctuation">,</span><br>            <span class="hljs-string">&quot;s3:DeleteObject&quot;</span><br>        <span class="hljs-punctuation">]</span><span class="hljs-punctuation">,</span><br>        <span class="hljs-attr">&quot;Resource&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">[</span><br>            <span class="hljs-string">&quot;arn:aws:s3:::your-bucket-name&quot;</span><span class="hljs-punctuation">,</span><br>            <span class="hljs-string">&quot;arn:aws:s3:::your-bucket-name/*&quot;</span><br>        <span class="hljs-punctuation">]</span><br>        <span class="hljs-punctuation">&#125;</span><br>    <span class="hljs-punctuation">]</span><br><span class="hljs-punctuation">&#125;</span><br></code></pre></td></tr></table></figure><p>Next, go to <code>IAM &gt; Users</code> to create a new user and attach the above policy. Once the user is created, issue Access Keys under <code>Command Line Interface</code> access.</p><p>Back to local terminal, use the following command to set up the profile:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">&gt; </span><span class="language-bash">aws configure</span><br>AWS Access Key ID [********************]:<br>AWS Secret Access Key [********************]:<br>Default region name [*************]: <br>Default output format [****]: <br></code></pre></td></tr></table></figure><p>For security, instead of hardcoding access keys, load them dynamically. Add this to <code>.bashrc</code> or <code>.zshrc</code>:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">export</span> AWS_ACCESS_KEY_ID=$(aws configure get aws_access_key_id)<br><span class="hljs-built_in">export</span> AWS_SECRET_ACCESS_KEY=$(aws configure get aws_secret_access_key)<br></code></pre></td></tr></table></figure><p>Apply the changes:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">source</span> ~/.zshrc<br></code></pre></td></tr></table></figure><p>S3 setup is now complete.</p><h2 id="Applying-duckdbrc"><a href="#Applying-duckdbrc" class="headerlink" title="Applying .duckdbrc"></a>Applying <code>.duckdbrc</code></h2><p>It’s tedious to reattach DBs and input env variables every time the UI restarts.<br>You can automate this by adding a <code>.duckdbrc</code> file in your home directory:</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs sql">ATTACH DATABASE &#123;<span class="hljs-operator">/</span>absolute<span class="hljs-operator">/</span>path<span class="hljs-operator">/</span><span class="hljs-keyword">to</span><span class="hljs-operator">/</span>.duckdb&#125;<span class="hljs-keyword">AS</span> dw;<br>INSTALL httpfs;<br>LOAD httpfs;<br><span class="hljs-keyword">SET</span> s3_region<span class="hljs-operator">=</span><span class="hljs-string">&#x27;ap-northeast-2&#x27;</span>;<br><span class="hljs-keyword">SET</span> s3_access_key_id<span class="hljs-operator">=</span><span class="hljs-string">&#x27;$&#123;AWS_ACCESS_KEY_ID&#125;&#x27;</span>;<br><span class="hljs-keyword">SET</span> s3_secret_access_key<span class="hljs-operator">=</span><span class="hljs-string">&#x27;$&#123;AWS_SECRET_ACCESS_KEY&#125;&#x27;</span>;<br></code></pre></td></tr></table></figure><p>This file automatically attaches the DB and sets S3 configs.<br>You’ll see the DB consistently attached every time you restart the UI:</p><p><img src="/en/images/10.png" alt="auto attached DB"></p><h2 id="Testing-S3-Table-in-DuckDB-UI"><a href="#Testing-S3-Table-in-DuckDB-UI" class="headerlink" title="Testing S3 Table in DuckDB UI"></a>Testing S3 Table in DuckDB UI</h2><p>Let’s prepare a sample dataset:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># Create sample.csv</span><br><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;id,name,country</span><br><span class="hljs-string">1,Alice,KR</span><br><span class="hljs-string">2,Bob,US</span><br><span class="hljs-string">3,Charlie,JP&quot;</span> &gt; sample.csv<br><br><span class="hljs-comment"># Upload to S3</span><br>aws s3 <span class="hljs-built_in">cp</span> sample.csv s3://your-bucket-name/sample.csv<br></code></pre></td></tr></table></figure><p>Try querying it:</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs text">HTTP Error: HTTP GET error on &#x27;https://your_bucket.s3.amazonaws.com/sample.csv&#x27; (HTTP 400)<br></code></pre></td></tr></table></figure><p>Uh-oh… the query fails!</p><p>Turns out, the <code>s3_region</code> setting in <code>.duckdbrc</code> wasn’t applied correctly.<br>After re-running the setting manually in the UI, the query works fine:</p><p><img src="/en/images/11.png" alt="query result"></p><p>I’ve reported the <a href="https://github.com/duckdb/duckdb-ui/issues/87">issue</a> in their GitHub repo.</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">CREATE TABLE</span> users <span class="hljs-keyword">AS</span><br><span class="hljs-keyword">SELECT</span> <span class="hljs-operator">*</span> <span class="hljs-keyword">FROM</span> read_csv_auto(<span class="hljs-string">&#x27;s3://your-bucket-name/sample.csv&#x27;</span>);<br></code></pre></td></tr></table></figure><p>Now, let’s create a <code>users</code> view based on the <code>CSV</code> file:</p><p><img src="/en/images/12.png" alt="create table"></p><p>Insert and Update the data:</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">UPDATE</span> users<br><span class="hljs-keyword">SET</span> name <span class="hljs-operator">=</span> <span class="hljs-string">&#x27;Bob Updated&#x27;</span>, country <span class="hljs-operator">=</span> <span class="hljs-string">&#x27;CA&#x27;</span><br><span class="hljs-keyword">WHERE</span> id <span class="hljs-operator">=</span> <span class="hljs-number">2</span>;<br><br><span class="hljs-keyword">INSERT INTO</span> users (id, name, country) <span class="hljs-keyword">VALUES</span> (<span class="hljs-number">4</span>, <span class="hljs-string">&#x27;David&#x27;</span>, <span class="hljs-string">&#x27;CN&#x27;</span>);<br></code></pre></td></tr></table></figure><p>These changes affect only the <code>local</code> users table, not the S3 file.<br>To persist the changes back to S3:</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">COPY</span> users <span class="hljs-keyword">TO</span> <span class="hljs-string">&#x27;s3://your-bucket-name/updated_users.parquet&#x27;</span> (FORMAT PARQUET);<br></code></pre></td></tr></table></figure><p><img src="/en/images/13.png" alt="create new table"></p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>Building your own data warehouse with DuckDB and S3 is surprisingly simple yet powerful.<br>The combination of DuckDB’s lightness and S3’s scalability allows you to set up a large-scale data analysis environment locally, without the need for dedicated cloud data warehouses.</p><p>One thing to keep an eye on is the .duckdbrc issue with <code>s3_region</code> not auto-applying.<br>Until it’s patched, manual configuration might still be necessary.</p>]]></content>
    
    
    <categories>
      
      <category>BigData</category>
      
      <category>engineering</category>
      
    </categories>
    
    
    <tags>
      
      <tag>duckdb</tag>
      
      <tag>datawarehouse</tag>
      
      <tag>data</tag>
      
      <tag>s3</tag>
      
      <tag>aws</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Do You Know About the Spark TimestampNTZ Data Type?</title>
    <link href="/en/en/Engineering/engineering-2/"/>
    <url>/en/en/Engineering/engineering-2/</url>
    
    <content type="html"><![CDATA[<p>Sharing an issue that occurred due to the introduction of the <code>TimestampNTZ</code> data type in environments using Spark and Delta.</p><span id="more"></span><h2 id="EMR-Version-Upgrade"><a href="#EMR-Version-Upgrade" class="headerlink" title="EMR Version Upgrade"></a>EMR Version Upgrade</h2><p>Recently, I upgraded the EMR version in use from 6.10 to 6.15. The reason for the urgent upgrade was an issue in Delta 2.2.0, included in EMR 6.10, where <code>alter</code> statements did not apply new changes. (For example, column names were not updated, or all values were returned as NULL.) If you are using Delta and experiencing the same issue, it is likely a bug in that specific version, so upgrading to a newer version is recommended.</p><p>With the EMR upgrade, the Spark and Delta versions were updated as follows:</p><ul><li>Spark: 3.3.0 → 3.4.1</li><li>Delta: 2.2.0 → 2.4.0</li></ul><p>Before proceeding, I checked the release notes for each version and concluded that there should be no issues since we were not using <code>TimestampNTZ</code>.</p><blockquote><p>Support the TimestampNTZ data type added in Spark 3.3. Using TimestampNTZ requires a Delta protocol upgrade; see the documentation for more information.</p></blockquote><p>The upgrade was completed, and as expected, there were no issues when testing with pyspark or SQL.</p><h2 id="Unexpected-Timestamp-Issue"><a href="#Unexpected-Timestamp-Issue" class="headerlink" title="Unexpected Timestamp Issue"></a>Unexpected Timestamp Issue</h2><p>The next day, our AI engineer reported an issue where <code>timestamp</code> values appeared incorrectly. After running a batch process with the same code as the previous day, he found that the timestamps were displayed in UTC. I quickly checked other batch jobs, but no similar issues were found.</p><p>To get straight to the point, the issue occurred when <code>timestamp</code> data was created in <code>pandas</code> and then inserted into a <code>delta</code> table.<br>Like many other platforms, our system stores data in UTC and converts it to KST only when reading. However, in this process, the data was converted to <code>UTC</code>, stored as <code>TimestampNTZ</code>, and then read as is in <code>Spark</code>.</p><p>That’s right—since <code>delta</code> started supporting <code>TimestampNTZ</code>, if a dataset has no explicit timezone information, the schema is inferred as this data type.</p><h2 id="What-is-the-TimestampNTZ-Data-Type"><a href="#What-is-the-TimestampNTZ-Data-Type" class="headerlink" title="What is the TimestampNTZ Data Type?"></a>What is the TimestampNTZ Data Type?</h2><ul><li>A new timezone-free data type supported in Spark 3.3+</li><li>Unlike the traditional timestamp type, it does not include timezone information and returns values exactly as stored</li><li>Officially supported in Delta 2.4.0+</li></ul><table><thead><tr><th>Data Type</th><th>Timezone Included</th><th>How Spark Converts</th></tr></thead><tbody><tr><td>Timestamp</td><td>Includes timezone</td><td>Converts to the configured timezone when queried</td></tr><tr><td>TimestampNTZ</td><td>Does not include timezone</td><td>Returns stored values as-is</td></tr></tbody></table><p>Therefore, if data is stored as this type, queries must explicitly cast it to a specific timezone.<br>However, if the timezone of the stored <code>TimestampNTZ</code> data is unknown, is casting even meaningful?</p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><ul><li>In Spark 3.3+ and Delta 2.4.0+, TimestampNTZ is enabled by default.</li><li>If pandas datetime data is inserted into Delta without a timezone setting, it is automatically stored as TimestampNTZ.</li><li>When querying TimestampNTZ data in Spark, time conversion does not happen automatically.</li></ul><p>Therefore, the data type should be explicitly defined before storage, or Spark should be configured to handle it explicitly.</p>]]></content>
    
    
    <categories>
      
      <category>BigData</category>
      
      <category>engineering</category>
      
    </categories>
    
    
    <tags>
      
      <tag>BigData</tag>
      
      <tag>Spark</tag>
      
      <tag>Delta</tag>
      
      <tag>TimestampNTZ</tag>
      
      <tag>DataType</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>My Experience Translating 『Julia for Data Analytics』</title>
    <link href="/en/en/Julia/julia-0/"/>
    <url>/en/en/Julia/julia-0/</url>
    
    <content type="html"><![CDATA[<p>In this post, I share my experience of translating an IT book for the first time — how I got started, the challenges I faced, and the lessons I learned along the way.</p><span id="more"></span><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>The book I translated was published in March 2024. Although this reflection comes quite late, I only recently found the time to look back on the experience as I settled down from my hectic routine. I’m not sure if I’ll have another chance to translate a book in the future, but just in case, I wanted to document both the difficulties and rewarding moments of the process.</p><p>Surprisingly, I was offered the opportunity to translate this book when the publisher reached out to me. In 2020, I was studied Julia and had written various blog posts about it. It seems the publisher discovered my posts and decided to contact me. Since it was a publisher whose IT books I had often purchased and read, I found the whole situation quite surreal. There was no reason for me to turn down such a great opportunity. I had a vague idea that it wouldn’t be hard to translate, because I’ve read so many tech articles and books in English. Without much hesitation, I immediately replied to the publisher’s team leader, expressing my interest in the project.</p><p><img src="/en/images/6.jpeg" alt="Plz Think more"></p><p>I should have thought about it more carefully.</p><h2 id="Challenges-During-Translation"><a href="#Challenges-During-Translation" class="headerlink" title="Challenges During Translation"></a>Challenges During Translation</h2><p>At first, I assumed I could approach the translation the same way I studied technical materials.<br>However, this was the naive confidence of someone who knew nothing about translation.</p><p>The first challenge was converting English technical terms into appropriate Korean equivalents. Many terms are commonly used in English within the industry, even in Korean-speaking contexts. For example, should ecosystem and named tuple be translated, or should they remain in English? How should I translate between parameter and argument in Korean? When should I include both English and Korean terms together (i.e., provide a transliteration or explanation)? Even choosing basic terms was more difficult than I thought.</p><p>English text is often structured with long, complex sentences, frequently using <code>which</code> or <code>that</code> to connect ideas. In contrast, Korean generally flows better when broken into two or three separate sentences. Additionally, English adjectives often follow nouns, whereas in Korean, they usually precede them. If I translated English text word-for-word, I ended up with what I call <code>heavy-headed sentences</code> that were difficult to read. However, if I broke up the sentences too much, I feared that I might alter the author’s intended meaning. Early on, I sometimes spent 30 minutes agonizing over a single sentence. But as I progressed, I naturally developed my own strategies and guidelines for handling these issues.</p><p>The third is something every office worker faces: the battle for time. Since my company is based at home, I thought I had a lot of time on my hands becuase i didn’t have to commute to the office. However, as an office worker, you have a busy day just managing your work and home. When work is busy, home is relaxed, and when work is relaxed, there are often incidents at home. I was no different. When I was busy at work, I was rushing to get things done, and when I was done, there were home issues that needed to be addressed, such as a sick family member who needed to be cared for. Translation requires long, uninterrupted sessions to ensure a smooth and consistent tone across chapters. With a looming deadline, I had to push through and complete the translation despite all these challenges.</p><h2 id="What-I-learned-from-translating"><a href="#What-I-learned-from-translating" class="headerlink" title="What I learned from translating"></a>What I learned from translating</h2><p>But if you ask me, “Do you regret working on the translation?”, of course not.<br>On the contrary, I feel honored to have been given this opportunity.</p><p>Translating forced me to revisit <code>basic concepts</code> that I had previously glossed over using industry jargon.<br>By putting these terms into my own words, I was able to internalize them more effectively and understand them much more deeply. I think the saying “if you really want to understand something, explain it in your own words” is so true.</p><p>In the process, my knowledge of <code>Julia</code> has greatly improved. Before this book, I learned Julia mainly by reading documentation and experimenting on my own. But this book systematically explains Julia’s ecosystem and practical usage, introducing me to many concepts I hadn’t encountered before. It also made me realize how much the language has evolved since I first started using it in 2020.</p><p>This experience gave me a glimpse into the world of <code>translation</code>, which I believe is a very rare and valuable opportunity. I gained first-hand knowledge of how book translation projects are initiated, negotiated, and executed, and gained insights that I would not have gotten otherwise.</p><h2 id="Final-Thoughts"><a href="#Final-Thoughts" class="headerlink" title="Final Thoughts"></a>Final Thoughts</h2><p>I am so grateful that I was able to have this experience, even though I have no experience in translation and am in a junior career in some ways. And also I’m grateful to the publisher and other people who worked with me on this project, even though I was a beginner in every aspect. When the next translation opportunity comes, I will study a lot so that I can do a better translation.</p><p>For those of you who are curious about what the translated book is like, here is the <a href="https://blog.naver.com/jeipubmarketer/223381130430">book introduction page</a>.</p>]]></content>
    
    
    <categories>
      
      <category>Language</category>
      
      <category>julia</category>
      
    </categories>
    
    
    <tags>
      
      <tag>julia</tag>
      
      <tag>translate</tag>
      
      <tag>translation</tag>
      
      <tag>dataanalysis</tag>
      
      <tag>JuliaforDataAnalysis</tag>
      
      <tag>book</tag>
      
      <tag>korean</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Please Build a Data Warehouse</title>
    <link href="/en/en/Engineering/engineering-1/"/>
    <url>/en/en/Engineering/engineering-1/</url>
    
    <content type="html"><![CDATA[<p>As a data engineer, I’d like to share the importance of a proper data warehouse and how to construct one effectively, based on my experience.</p><span id="more"></span><h2 id="Introduction-Pouring-Water-into-a-Bottomless-Pit"><a href="#Introduction-Pouring-Water-into-a-Bottomless-Pit" class="headerlink" title="Introduction: Pouring Water into a Bottomless Pit"></a>Introduction: Pouring Water into a Bottomless Pit</h2><p>It’s already been four years since I started my career as a data engineer, and I’ve been working at my current company for almost three years now. When I first joined, the system relied heavily on duplicating RDS for creating data products. But over time, we’ve stabilized our data lake using the Medallion architecture, allowing us to focus on improving the quality of data products.</p><p>In this post, I’ll summarize the efforts made throughout 2024 to build a data warehouse and improve data quality.</p><p>From conversations with others, I’ve often heard the following reasons why data warehouses are perceived as a double-edged sword:</p><ul><li>Most data teams are small and too busy to handle such projects.</li><li>Setting up and properly using a data warehouse requires significant resources.</li><li>Adding another layer before the product feels unnecessarily cumbersome.</li><li>Some people still don’t fully understand why it’s needed given the resources required.</li></ul><p>So some companies didn’t have one at all, and most of them just organized it as One Big Table (OBT) or preprocessed the data types of frequently used tables and called it a data warehouse.</p><p>Our company was no exception. Without a proper data warehouse, dashboards and metrics were calculated differently by per people, leading to inconsistent results across products. Meanwhile, technical debt kept piling up.</p><p>In such an environment, constantly maintaining systems felt like <code>pouring water into a bottomless pit</code>.</p><p><img src="/en/images/5.png" alt="WHAT I DO"></p><h2 id="Why-Do-We-Need-a-Data-Warehouse"><a href="#Why-Do-We-Need-a-Data-Warehouse" class="headerlink" title="Why Do We Need a Data Warehouse?"></a>Why Do We Need a Data Warehouse?</h2><p>The absence of a data warehouse leads to the following major issues:</p><ol><li><p>Data quality management becomes challenging.</p><ul><li>Developers apply different metric definitions, causing confusion.</li><li>Changes in backend structures are often not communicated to the data team, leading to outdated data being used.</li><li>When metric logic changes, it’s hard to identify all the areas that need updates.</li></ul></li><li><p>Managing pipeline code becomes difficult.</p><ul><li>With everyone creating their own pipelines, maintenance becomes impossible if someone leaves the company.</li><li>Even within the data team, it’s unclear what products exist.</li><li>Maintenance consumes excessive time and resources.</li></ul></li></ol><p>A data warehouse plays a central role in managing data quality and maintaining efficient code. For example, when defining GMV for our company, some people calculated it using <code>pay_time</code>, while others used <code>create_time</code>. Such inconsistencies erode trust and reliability in the data. By defining a standard GMV formula and adding a GMV column to a warehouse table, we ensured all products pulled the value from the same source. Later, if the GMV calculation formula changes, updating the warehouse pipeline alone ensures consistency across all metrics.</p><p>Additionally, when backend logic changes or server issues cause data integrity problems, the warehouse manager can quickly address them. Without this centralization, different data products with independent pipelines create an exponential increase in maintenance tasks, rendering the system unmanageable. Over time, this results in significant legacy systems—akin to museum relics—that exist solely for troubleshooting. A data warehouse prevents such issues and provides an excellent solution.</p><p>Our company decided to address these challenges by building a proper data warehouse.</p><h2 id="How-to-Build-a-Data-Warehouse"><a href="#How-to-Build-a-Data-Warehouse" class="headerlink" title="How to Build a Data Warehouse"></a>How to Build a Data Warehouse</h2><p>So, how do you get started with building a data warehouse? The first step is to <code>understand the service domains</code>. In our company, multiple domains exist within a single large service, each with distinct business logic. Understanding these business rules is essential to define the scope during data modeling.</p><p>The next step is to <code>understand the operational data</code> tied to the business logic. Analytical data is fundamentally derived from service-generated data. Without understanding the service database logic, building a warehouse is impossible. This involves consulting with the backend team, reading historical documentation, and directly querying and exploring the schemas.</p><p>Once you’ve studied the data, you can move to the development phase.</p><p>The third step is to <code>design your data modeling</code>. This involves choosing between a star schema and a snowflake schema based on the business logic and data. For more details on these schema structures, check out <a href="https://dev-bearabbit.github.io/en/en/Engineering/engineering-0/">this post</a>.</p><p>Some tips to keep in mind during modeling:</p><ul><li>Define common metrics first and modeling around them.</li><li>Use logical primary keys for table integration.</li><li>Add data integrity labels (e.g., VALID, BAD_DATA, INVALID).</li><li>Document your data model thoroughly.</li><li>Include separate columns for INSERT and UPDATE timestamps.</li></ul><p>Once you’re done modeling your data, the next step is to <code>design your ETL/ELT pipeline</code>. The pipeline should reflect the modeling results, supporting either INSERT or UPDATE operations. Choosing a platform that supports UPSERT is ideal. Our team uses Spark with the open-source Delta Lake library to implement UPSERT operations. Additionally, we set up Airflow to run daily batches, ensuring that the latest data is regularly updated or added. Creating pipeline templates can save time when configuring DAGs repeatedly.</p><p>Next, you need to configure an environment to <code>organize and share the table schemas in your warehouse</code>. Organizing and sharing table schemas within the warehouse is essential. Usually, a data catalog platform is used for this purpose. Our team uses DataHub to manage and share schemas. Without documenting tables during modeling, you’ll face a mountain of overdue documentation tasks later.</p><p>The final step is to <code>organize your quality control and testing system</code>. The warehouse assumes its data is high-quality by default, so maintaining quality control is crucial. Engineers responsible for the warehouse should continually think about quality and testing. This includes:</p><ul><li>Data integrity tests</li><li>NULL value checks</li><li>Duplicate data removal</li><li>Monitoring historical changes and outliers</li></ul><p>In reality, business logic and operational data logic constantly evolve as services grow and change. Ultimately, data quality management involves recognizing, understanding, and consistently reflecting these changes. For this reason, building and maintaining a data warehouse requires significant resources. But if you build a product without these filters in place, you’re more likely to become a mass-production shop that accumulates technical debt that’s hard to reverse at scale.</p><p>To summarize, the process can be broken down as follows:</p><ol><li><p>Learning Phase</p><ul><li>Understand the business logic for each service domain.</li><li>Understand the operational data and database logic for the service.</li></ul></li><li><p>Development Phase</p><ul><li>Design the data model.</li><li>Design and develop the ETL&#x2F;ELT pipelines.</li><li>Build a data catalog.</li><li>Establish quality control and testing systems.</li></ul></li></ol><h2 id="Conclusion-Please-Build-a-Data-Warehouse"><a href="#Conclusion-Please-Build-a-Data-Warehouse" class="headerlink" title="Conclusion: Please Build a Data Warehouse"></a>Conclusion: Please Build a Data Warehouse</h2><p>A data warehouse forms the backbone of a data team, ensuring data quality and simplifying code maintenance. Addressing the aforementioned issues and goals can significantly enhance a data team’s productivity and efficiency.</p><p>A data warehouse is not just a storage solution but the heart of the data team, systematically managing data and providing reliable information. If infrastructure like this isn’t built when the system is manageable, the team will likely descend into maintenance hell.</p><p>So, the conclusion of this post is simple: please build a data warehouse early to prevent chaos down the road.</p>]]></content>
    
    
    <categories>
      
      <category>BigData</category>
      
      <category>engineering</category>
      
    </categories>
    
    
    <tags>
      
      <tag>BigData</tag>
      
      <tag>DataWarehouse</tag>
      
      <tag>DW</tag>
      
      <tag>DataQuality</tag>
      
      <tag>Pipeline</tag>
      
      <tag>DataEngineering</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Avoid a Cost Explosion with AWS Personalize</title>
    <link href="/en/en/Lesson/lesson-0/"/>
    <url>/en/en/Lesson/lesson-0/</url>
    
    <content type="html"><![CDATA[<p>In this post, I’ll share how I unexpectedly faced a cost explosion while using AWS Personalize. Hopefully, my experience can help others avoid similar pitfalls in SaaS cost management.</p><span id="more"></span><hr><h2 id="The-Incident"><a href="#The-Incident" class="headerlink" title="The Incident"></a>The Incident</h2><p>I was developing and testing a tool for extracting CRM campaign audiences. As part of this process, I compared AWS Personalize pricing, estimated the expected costs, and documented everything in a Confluence file to present to my lead. After approval, we moved forward with A&#x2F;B testing between our current method and the model output. For context, I used the <code>aws-item-affinity</code> recipe for item-user segmentation.</p><p>However, this morning, I received an alert about a sudden surge in Personalize costs. According to my calculations, the expenses should have been minimal. What went wrong? My spine chilled as I felt something was terribly off. Feeling a wave of panic, I immediately began investigating.</p><p><img src="/en/images/0.png" alt="What’s going on"></p><p>I spent the entire morning staring at the <a href="https://aws.amazon.com/personalize/pricing/">AWS Personalize pricing page</a> and recalculating costs based on usage. Finally, I identified the issue.</p><h2 id="Root-Cause"><a href="#Root-Cause" class="headerlink" title="Root Cause"></a>Root Cause</h2><p>The culprit was the batch segment job. While the dataset and training costs were minimal since they involved one-time charges, the batch segment cost was the bombshell. Let’s take a look at the pricing details from AWS’s official page.</p><p><img src="/en/images/1.png" alt="Pricing table for batch segment"></p><p>The table above outlines how batch segment costs are calculated. Here’s where I misunderstood: I assumed the term Users in dataset referred to the number of users requested per segment. That is, for up to 100,000 requests, the cost would be <code>$0.016</code> per segment request, and for 100,001–900,000 requests, the cost would drop to <code>$0.008</code>. Based on this, I calculated the cost for retrieving 10,000 users for each of 10 items like this:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-number">10000</span>/<span class="hljs-number">1000</span> * <span class="hljs-number">0.016</span> * <span class="hljs-number">10</span><br></code></pre></td></tr></table></figure><p>However, <code>Users in dataset</code> actually refers to <code>Total Users in the Dataset</code>—the total number of users in the training dataset—irrespective of the number of users requested in the segment job. Since I trained the model on a year’s worth of data, the dataset contained approximately 600,000 users. The correct calculation, then, is:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">Tier_1 = <span class="hljs-number">100000</span>/<span class="hljs-number">1000</span> * <span class="hljs-number">0.016</span> * <span class="hljs-number">10</span><br>Tier_2 = <span class="hljs-number">500000</span>/<span class="hljs-number">1000</span> * <span class="hljs-number">0.008</span> * <span class="hljs-number">10</span><br></code></pre></td></tr></table></figure><p>In my initial understanding, the cost would have been just <code>$1.6</code>, but the actual cost came to <code>$56</code>. This was entirely my mistake for not double-checking the details. Looking back, I should have questioned why the costs seemed so low and revisited the documentation for clarity. At the same time, I can’t help but feel like AWS’s pricing table is designed to be confusing—it never crossed my mind that the requested number of users wouldn’t factor into the calculation at all.</p><p><img src="/en/images/2.png" alt="Plz undo everything"></p><p>If you miscalculate the cost of using SaaS in the cloud ecosystem, you’ll end up paying a lot of unnecessary money, even if your app works. I wrote my first postmortem documentation as a developer because of this cost. I felt sad because I made a mistake I didn’t expect. For anyone confused like me when calculating costs, I’m leaving this on my blog.</p><h2 id="Lessons-Learned"><a href="#Lessons-Learned" class="headerlink" title="Lessons Learned"></a>Lessons Learned</h2><ul><li>Cloud costs are seriously scary. Sometimes, I wonder if server crashes would be stressful… Actually, no—that’s a nightmare too.</li><li>Always read AWS documentation carefully and double-check with others to avoid misunderstandings.</li><li>For AWS Personalize, pay close attention to segment costs. The size of the training dataset directly affects batch segment charges.</li></ul>]]></content>
    
    
    <categories>
      
      <category>etc.</category>
      
      <category>lessons</category>
      
    </categories>
    
    
    <tags>
      
      <tag>aws</tag>
      
      <tag>personalize</tag>
      
      <tag>pricing</tag>
      
      <tag>mistake</tag>
      
      <tag>aws-item-affinity</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Customizing Hexo for a Multilingual Blog</title>
    <link href="/en/en/AboutHexo/about-hexo-0/"/>
    <url>/en/en/AboutHexo/about-hexo-0/</url>
    
    <content type="html"><![CDATA[<p>Here’s how to customize your Hexo blog to support multiple languages.</p><span id="more"></span><hr><h2 id="Current-Status"><a href="#Current-Status" class="headerlink" title="Current Status"></a>Current Status</h2><p>Here’s the current state of my technical blog:</p><ul><li>Hexo with the Fluid theme is being used.</li><li>The Fluid theme does not natively support multiple languages.</li><li>Custom development is needed to enable multilingual functionality.</li></ul><h2 id="Customization-Requirements"><a href="#Customization-Requirements" class="headerlink" title="Customization Requirements"></a>Customization Requirements</h2><p>The customization requirements were as follows:</p><ul><li>The root URL should serve the Korean version, and the English version should be redirected to the &#x2F;en&#x2F; path.</li><li>Posts should be written independently for each language, with categories and tags tailored to the specific language.</li><li>A language selection dropdown should be added to the navbar, allowing users to switch languages.</li></ul><h2 id="Customization-Setup"><a href="#Customization-Setup" class="headerlink" title="Customization Setup"></a>Customization Setup</h2><p>The customization process was straightforward: create <strong>two separate Hexo blogs</strong> using the same theme and modify only the navbar.<br>To meet the requirements, the blog deployment and source code folders were structured as follows:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">drwxr-xr-x  18 ryu  staff   576  1  2 06:33 deploy<br>drwxr-xr-x  13 ryu  staff   416  1  2 05:22 en-blog<br>drwxr-xr-x  11 ryu  staff   352  1  3 01:47 ko-blog<br></code></pre></td></tr></table></figure><ul><li><code>en-blog</code>: Source code for the English blog</li><li><code>ko-blog</code>: Source code for the Korean blog</li><li><code>deploy</code>: Final folder containing files ready for deployment</li></ul><p>If you’re starting from scratch, create the blog source folders using the following commands:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">hexo init ko-blog<br>hexo init en-blog<br></code></pre></td></tr></table></figure><p>Then navigate to each folder, download the desired theme, and set the default configuration separately.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># Korean version</span><br><span class="hljs-built_in">cd</span> ko-blog<br>npm install --save hexo-theme-fluid<br><br><span class="hljs-comment"># English version</span><br><span class="hljs-built_in">cd</span> en-blog<br>npm install --save hexo-theme-fluid<br></code></pre></td></tr></table></figure><p>Since I planned to use the Korean version as the root URL, I configured the <code>_config.yml</code> files as follows:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># Korean version: /ko-blog/_config.yml</span><br>language: ko<br>url: https://dev-bearabbit.github.io<br>root: /<br>permalink: :lang/:title/<br><br><span class="hljs-comment"># English version: /en-blog/_config.yml</span><br>language: en<br>root: /en/<br>permalink: :lang/:title/<br></code></pre></td></tr></table></figure><p>This configuration adds the lang parameter to the permalink, so it’s essential to include lang in each post. To simplify this, I modified the default post scaffolds for each blog:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># Korean version: /ko-blog/scaffolds/post.md</span><br>---<br>title: &#123;&#123; title &#125;&#125;<br>lang: ko<br><span class="hljs-built_in">date</span>: &#123;&#123; <span class="hljs-built_in">date</span> &#125;&#125;<br>tags:<br>categories:<br>---<br><br><span class="hljs-comment"># English version: /en-blog/scaffolds/post.md</span><br>---<br>title: &#123;&#123; title &#125;&#125;<br>lang: en<br><span class="hljs-built_in">date</span>: &#123;&#123; <span class="hljs-built_in">date</span> &#125;&#125;<br>tags:<br>categories:<br>---<br></code></pre></td></tr></table></figure><p>Next, I added a language selection dropdown to the navbar by editing the theme’s navigation files. In the Fluid theme, this can be done in <code>layout/_partials/header/navigation.ejs</code>.</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs html"><span class="hljs-comment">&lt;!-- Korean version: /ko-blog/themes/fluid/layout/_partials/header/navigation.ejs --&gt;</span><br><span class="hljs-comment">&lt;!-- English version: /en-blog/themes/fluid/layout/_partials/header/navigation.ejs --&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">div</span> <span class="hljs-attr">class</span>=<span class="hljs-string">&quot;dropdown-menu&quot;</span> <span class="hljs-attr">aria-labelledby</span>=<span class="hljs-string">&quot;languageDropdown&quot;</span>&gt;</span><br>  <span class="hljs-tag">&lt;<span class="hljs-name">a</span> <span class="hljs-attr">class</span>=<span class="hljs-string">&quot;dropdown-item&quot;</span> <span class="hljs-attr">href</span>=<span class="hljs-string">&quot;/&quot;</span>&gt;</span>한국어<span class="hljs-tag">&lt;/<span class="hljs-name">a</span>&gt;</span><br>  <span class="hljs-tag">&lt;<span class="hljs-name">a</span> <span class="hljs-attr">class</span>=<span class="hljs-string">&quot;dropdown-item&quot;</span> <span class="hljs-attr">href</span>=<span class="hljs-string">&quot;/en/&quot;</span>&gt;</span>English<span class="hljs-tag">&lt;/<span class="hljs-name">a</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">div</span>&gt;</span><br></code></pre></td></tr></table></figure><p>To display the selected language in the navbar, I added the following script:</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs html"><span class="hljs-tag">&lt;<span class="hljs-name">script</span>&gt;</span><span class="language-javascript"></span><br><span class="language-javascript">  <span class="hljs-variable language_">document</span>.<span class="hljs-title function_">addEventListener</span>(<span class="hljs-string">&#x27;DOMContentLoaded&#x27;</span>, <span class="hljs-function">() =&gt;</span> &#123;</span><br><span class="language-javascript">    <span class="hljs-keyword">const</span> path = <span class="hljs-variable language_">window</span>.<span class="hljs-property">location</span>.<span class="hljs-property">pathname</span>;</span><br><span class="language-javascript">    <span class="hljs-keyword">const</span> currentLang = path.<span class="hljs-title function_">startsWith</span>(<span class="hljs-string">&#x27;/en/&#x27;</span>) ? <span class="hljs-string">&#x27;English&#x27;</span> : <span class="hljs-string">&#x27;한국어&#x27;</span>;</span><br><span class="language-javascript">    <span class="hljs-keyword">const</span> currentLangElement = <span class="hljs-variable language_">document</span>.<span class="hljs-title function_">getElementById</span>(<span class="hljs-string">&#x27;current-lang&#x27;</span>);</span><br><span class="language-javascript">    currentLangElement.<span class="hljs-property">textContent</span> = currentLang;</span><br><span class="language-javascript">  &#125;);</span><br><span class="language-javascript"></span><span class="hljs-tag">&lt;/<span class="hljs-name">script</span>&gt;</span><br></code></pre></td></tr></table></figure><p>Finally, I updated the <code>head.ejs</code> file to include alternate language links:</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs html"><span class="hljs-comment">&lt;!-- Korean version: /ko-blog/themes/fluid/layout/_partials/head.ejs --&gt;</span><br><span class="hljs-comment">&lt;!-- English version: /en-blog/themes/fluid/layout/_partials/head.ejs --&gt;</span><br><br><span class="hljs-tag">&lt;<span class="hljs-name">link</span> <span class="hljs-attr">rel</span>=<span class="hljs-string">&quot;alternate&quot;</span> <span class="hljs-attr">hreflang</span>=<span class="hljs-string">&quot;ko&quot;</span> <span class="hljs-attr">href</span>=<span class="hljs-string">&quot;https://dev-bearabbit.github.io/&quot;</span> /&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">link</span> <span class="hljs-attr">rel</span>=<span class="hljs-string">&quot;alternate&quot;</span> <span class="hljs-attr">hreflang</span>=<span class="hljs-string">&quot;en&quot;</span> <span class="hljs-attr">href</span>=<span class="hljs-string">&quot;https://dev-bearabbit.github.io/en/&quot;</span> /&gt;</span><br></code></pre></td></tr></table></figure><p>With these changes, the customization is complete.</p><h2 id="Deploying-the-Blog"><a href="#Deploying-the-Blog" class="headerlink" title="Deploying the Blog"></a>Deploying the Blog</h2><p>The blog is deployed using GitHub Pages. First, generate the deployment files for both blogs:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># Korean version: ko-blog</span><br><span class="hljs-comment"># English version: en-blog</span><br>hexo clean<br>hexo generate<br></code></pre></td></tr></table></figure><p>The generated files will be located in the following directories:</p><ul><li>Korean version: ko-blog&#x2F;public&#x2F;</li><li>English version: en-blog&#x2F;public&#x2F;</li></ul><p>Move the files into the <code>deploy</code> folder. Ensure the Korean version is moved to the root of <code>deploy</code>, while the English version is moved to <code>deploy/en/</code>.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">rm</span> -rf deploy/*<br><br><span class="hljs-built_in">mv</span> ko-blog/public/* deploy/<br><span class="hljs-built_in">mkdir</span> -p deploy/en/ &amp;&amp; <span class="hljs-built_in">mv</span> en-blog/public/* deploy/en/<br></code></pre></td></tr></table></figure><p>Finally, deploy the contents of the deploy folder to GitHub.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># Initial setup</span><br><span class="hljs-built_in">cd</span> deploy<br>git init<br>git remote add origin https://github.com/&lt;username&gt;/&lt;repository&gt;.git<br><br><span class="hljs-comment"># Push changes</span><br>git add .<br>git commit -m <span class="hljs-string">&quot;Deploy folder initial commit&quot;</span><br>git push origin main<br></code></pre></td></tr></table></figure><p>This completes the setup for a multilingual Hexo blog.</p>]]></content>
    
    
    <categories>
      
      <category>etc.</category>
      
      <category>about hexo</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Hexo</tag>
      
      <tag>blog</tag>
      
      <tag>multilingual</tag>
      
      <tag>custom</tag>
      
      <tag>English blog</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Basic Structure of Analytical Tables</title>
    <link href="/en/en/Engineering/engineering-0/"/>
    <url>/en/en/Engineering/engineering-0/</url>
    
    <content type="html"><![CDATA[<p>Before I begin, I’d like to note that this article is summarized and organized from Keisuke Nishida’s book Big Data Technology.</p><span id="more"></span><h2 id="Normalization-and-Denormalization"><a href="#Normalization-and-Denormalization" class="headerlink" title="Normalization and Denormalization"></a>Normalization and Denormalization</h2><p>Typically, relational databases(RDBs) are structured using normalized data, which is generally divided into transaction data and master data. <code>Transaction data</code> refers to data that is recorded sequentially over time, while <code>Master data</code> contains various types of reference information used by transaction data. Transaction data is immutable once recorded, whereas master data may change depending on circumstances.</p><p>The process of organizing data to minimize redundancy is called <code>Normalization</code>. By normalizing data, integrity can be maintained, and the storage size can be optimized.</p><p>However, tables used for analysis often undergo the opposite process called <code>Denormalization</code>. Denormalization involves combining transaction data and master data to make analysis more convenient. To create dimension tables for analysis, normalized tables are recombined into a single table. In this process, data redundancy is acceptable.</p><h2 id="Multidimensional-Model"><a href="#Multidimensional-Model" class="headerlink" title="Multidimensional Model"></a>Multidimensional Model</h2><p>A multidimensional model, as the name suggests, represents data across multiple dimensions. Normally, when we think of data, we envision a two-dimensional model consisting of rows and columns. However, operational data often has much more complex structures, making it challenging to represent it simply in two dimensions. Attempting to do so could result in significant data duplication and an unmanageable number of columns.</p><p>In industry terms, a multidimensional model refers to a design where data is divided into <code>fact tables</code>, which represent actual data, and <code>dimension tables</code>, which provide reference values. This structure minimizes redundancy and helps users better understand the scope of the data.</p><p>Multidimensional models are typically designed based on the metrics to be analyzed. On the other hand, MPP(Massively Parallel Processing) databases do not incorporate the concept of multidimensional models, instead relying on denormalized tables.</p><h2 id="Fact-Table-and-Dimension-Table"><a href="#Fact-Table-and-Dimension-Table" class="headerlink" title="Fact Table and Dimension Table"></a>Fact Table and Dimension Table</h2><p><code>Fact tables</code> record transactional data or observations, while <code>Dimension tables</code> store the reference data used by the fact tables. For example, numerical data used for aggregation is stored in fact tables, and dimension tables are used to classify this data by providing attributes.</p><p>As data volume grows, fact tables become significantly larger than dimension tables, and the size of fact tables directly affects aggregation performance. Therefore, keeping fact tables as small as possible is crucial for high performance. Only keys like IDs are retained in fact tables, with other attributes moved to dimension tables.</p><h2 id="Star-Schema-and-Snowflake-Schema"><a href="#Star-Schema-and-Snowflake-Schema" class="headerlink" title="Star Schema and Snowflake Schema"></a>Star Schema and Snowflake Schema</h2><p>The <code>Star Schema</code> organizes data with a fact table at the center, surrounded by multiple dimension tables. Dimension tables are single-layered and do not have hierarchical structures. The following image illustrates the structure of a star schema:</p><p><img src="/en/images/3.png" alt="star schema"></p><p>The <code>Snowflake Schema</code> extends the star schema by allowing dimension tables to have hierarchical sub-dimensions. In other words, it applies stronger normalization compared to the star schema. The following image shows the structure of a snowflake schema:</p><p><img src="/en/images/4.png" alt="snowflake schmea"></p><p>The comparison between the star schema and the snowflake schema is as follows:</p><table><thead><tr><th>Aspect</th><th>Star Schema</th><th>Snowflake Schema</th></tr></thead><tbody><tr><td>Advantages</td><td>- Simple data structure and queries. <br></td><td>- Minimizes data redundancy and storage space. <br> - Easier to maintain due to the absence of redundancy.</td></tr><tr><td>Disadvantages</td><td>- Dimension tables may have redundancy. <br> - Requires more storage space.</td><td>- Requires more joins, leading to potential performance issues. <br> - More complex data structure.</td></tr></tbody></table><p>Generally, the snowflake schema is more suitable for data warehouses, while the star schema is considered better for data marts.</p><h2 id="Columnar-Storage-and-Denormalized-Tables"><a href="#Columnar-Storage-and-Denormalized-Tables" class="headerlink" title="Columnar Storage and Denormalized Tables"></a>Columnar Storage and Denormalized Tables</h2><p>With the advent of columnar storage systems, the situation has changed. Even if the number of columns increases, the performance of query engines is not significantly affected, and column-level compression reduces disk I&#x2F;O overhead. Consequently, there is less need to separate data into dimension tables, and a single, large fact table may suffice.</p><p>A <code>Denormalized table</code> is created by further denormalizing a star schema and combining all the tables into one fact table. For data warehouses, the star schema remains optimal for table structures. During the data accumulation phase, separating fact and dimension tables is preferable, but when creating data marts, combining them into a denormalized table is often recommended.</p>]]></content>
    
    
    <categories>
      
      <category>BigData</category>
      
      <category>engineering</category>
      
    </categories>
    
    
    <tags>
      
      <tag>BigData</tag>
      
      <tag>DataMart</tag>
      
      <tag>DataWarehouse</tag>
      
      <tag>Schema</tag>
      
      <tag>StarSchema</tag>
      
      <tag>SnowflakeSchema</tag>
      
      <tag>DenormalizedTable</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
